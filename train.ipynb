{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e40039-4950-48e2-bfd1-0a60fc9e6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#sys.path.append(\"/xddata/home/acg1fa4eq1/.conda/envs/unet3d/lib/python3.7/site-packages\")\n",
    "sys.path.append(\"/public/software/apps/anaconda3/5.2.0/envs/pytorch1.10.0a0-py37-dtk22.04.2/lib/python3.7/site-packages\")\n",
    "sys.path.append(\"/public/home/msliuzy/.local/lib/python3.7/site-packages\")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import MyDataset, cal_acc_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75b7df7-0a36-44a6-92a0-44689dd90f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "840\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"dataset/train\"\n",
    "val_dir = \"dataset/val\"\n",
    "# data max: 157.5, min: -319.75\n",
    "# data mean: 0.669921875, std: 0.007345963568431114\n",
    "\n",
    "mean = 0.669921875\n",
    "std = 0.007345963568431114\n",
    "_max = 157.5\n",
    "_min = -319.75\n",
    "train_dataset = MyDataset(train_dir,mean=mean,std=std)\n",
    "val_dataset = MyDataset(val_dir,mean=mean,std=std)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b51861f-0cea-4511-8691-449690052f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from utils import mse_ce_loss\n",
    "\n",
    "def train_model(model, train_loader, val_loader, n_epochs, \n",
    "                learning_rate,\n",
    "                min_lr=1e-10, \n",
    "                decay_steps=20,\n",
    "                lamda=0.5,\n",
    "                is_ae=True,\n",
    "                ckpt_dir=\"best-ckpt.pt\"):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "    criterion = nn.L1Loss(reduction='sum').to(device)\n",
    "    \n",
    "    gamma = 0.5\n",
    "    #scheduler = lr_scheduler.StepLR(optimizer, step_size=decay_steps, gamma=gamma)\n",
    "    scheduler = CosineAnnealingLR(optimizer = optimizer, T_max = 40, eta_min=1e-5)\n",
    "    \n",
    "    milestones = [2, 5, 10, 20, 25, 30, 35, 40, 50, 60, 70, 80 ] \n",
    "    gamma = 0.5\n",
    "    #scheduler = lr_scheduler.MultiStepLR(optimizer, milestones, gamma)\n",
    "    \n",
    "    \n",
    "    # criterion  = nn.MSELoss().to(device)\n",
    "    history = dict(train=[], val=[])\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 10000.0\n",
    "    \n",
    "    # 加载best_ckpt\n",
    "    if os.path.exists(ckpt_dir):\n",
    "        print(f'load pretrained model from {ckpt_dir}')\n",
    "        model.load_state_dict(torch.load(ckpt_dir))\n",
    "    else:\n",
    "        print(\"traing start...\")\n",
    "  \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model = model.train()\n",
    "\n",
    "        train_losses = []\n",
    "        \n",
    "        correct_sum = 0\n",
    "        count = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda().long()\n",
    "\n",
    "\n",
    "\n",
    "            if is_ae:\n",
    "                output, _labels = model(inputs)\n",
    "\n",
    "                mse_loss, ce_loss = mse_ce_loss(output, inputs, _labels, labels)\n",
    "                loss = mse_loss * lamda + ce_loss * ( 1 - lamda)\n",
    "            else:\n",
    "                \n",
    "                _labels = model(inputs)\n",
    "\n",
    "                if len(_labels.shape) == 1:\n",
    "                    _labels  = _labels.view(1, -1)\n",
    "                loss = torch.nn.CrossEntropyLoss(reduction='sum')(_labels, labels)\n",
    "\n",
    "            #loss = ce_loss\n",
    "            \n",
    "            pred = _labels.max(1, keepdim=True)[1]\n",
    "            \n",
    "            correct = pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "            \n",
    "            correct_sum += correct\n",
    "            count += len(labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item() / len(labels))\n",
    "            print(f'Batch Loss: {loss.item() / len(labels):.4f} Acc: {round(correct / len(labels)* 100, 2)}', end='\\r')\n",
    " \n",
    "        val_losses = []\n",
    "        correct_val = 0\n",
    "        count_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model = model.eval()\n",
    "            for inputs,labels in val_loader:\n",
    "\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda().long()\n",
    "\n",
    "                if len(_labels.shape) == 1:\n",
    "                    _labels = _labels.view(1, -1)\n",
    "                #print(_labels.shape,labels.shape)\n",
    "                \n",
    "                if is_ae:\n",
    "                    output, _labels = model(inputs)\n",
    "\n",
    "                    mse_loss, ce_loss = mse_ce_loss(output, inputs, _labels, labels)\n",
    "                    loss = mse_loss * lamda + ce_loss * ( 1 - lamda)\n",
    "                else:\n",
    "\n",
    "                    _labels = model(inputs)\n",
    "\n",
    "\n",
    "                    if len(_labels.shape) == 1:\n",
    "                        _labels = _labels.view(1, -1)\n",
    "                    loss = torch.nn.CrossEntropyLoss(reduction='sum')(_labels, labels)\n",
    "\n",
    "                \n",
    "                pred = _labels.max(1, keepdim=True)[1]\n",
    "                #print(_labels)\n",
    "                correct_val += pred.eq(labels.view_as(pred)).sum().item()\n",
    "                \n",
    "\n",
    "                count_val += len(labels)\n",
    "                \n",
    "                val_losses.append(loss.item() / len(labels))\n",
    "\n",
    "                \n",
    " \n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        \n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "         \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = max(param_group['lr'], min_lr)\n",
    "            lr = param_group['lr'] \n",
    "                              \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        train_acc = round(correct_sum / count* 100, 2)\n",
    "        val_acc = round(correct_val / count_val* 100, 2)\n",
    "\n",
    "        print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}  train acc {train_acc}  val acc {val_acc} lr: {lr}')\n",
    "        scheduler.step()\n",
    "        if epoch > 1 and epoch % 20 == 0:\n",
    "            print('Saving model...', end='\\r')\n",
    "            torch.save(best_model_wts,ckpt_dir)\n",
    "    \n",
    "\n",
    "            #model.load_state_dict(best_model_wts)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80f34b22-32ce-40f0-b179-edb533b182e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_LSTM(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(1, 128, kernel_size=(20,), stride=(3,), padding=(10,))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool1d(kernel_size=2, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(128, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv1d(64, 64, kernel_size=(10,), stride=(1,), padding=(5,))\n",
      "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LSTM(64, 64, batch_first=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "    (2): Linear(in_features=64, out_features=60, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=60, out_features=20, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=20, out_features=4, bias=True)\n",
      "    (7): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import CNN_LSTM\n",
    "from model import ResNet, ResBlk\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#############################################################################################################\n",
    "# 网络参数  Created by Lcy  2023.10.25\n",
    "#############################################################################################################\n",
    "# BestAcc     Model           Params              bidirectional-LSTM             File\n",
    "#  83        CNN_LSTM         128 64 60 20              No                   CNN_LSTM_128_64_60_20_acc83.pt\n",
    "#  80        Se-ResNet        blk[1 2 2 1]              Yes                  Se-ResNet_1221_acc80.pt\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "# CNN_LSTM参数\n",
    "\n",
    "input_dim = 1  # 输入信号维度\n",
    "hidden_dim = 128 # cnn \n",
    "hidden_size = 64 # lstm\n",
    "fc=[60,20] # fc层网络\n",
    "squeeze = False # 启用SeBlock\n",
    "\n",
    "\n",
    "drop_out = 0.1\n",
    "n_classes = 4\n",
    "\n",
    "\n",
    "model = CNN_LSTM(in_dims=input_dim, hidden_dims=hidden_dim, hidden_size=hidden_size,  num_classes=n_classes, fc=fc, squeeze=squeeze)\n",
    "\n",
    "#model = ResNet(ResBlk,[1,2,2,1],num_classes=n_classes,cnn_channels=[24,8,8,8],lstm_dims=[32,32,1], include_top=True)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "251a93cc-8b28-4db9-bc13-00aa6813b90f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained model from best-ckpt.pt\n",
      "Epoch 1: train loss 0.18780831836518788 val loss 0.6423427691433302  train acc 92.86  val acc 78.06 lr: 0.0001\n",
      "Epoch 2: train loss 0.17805484590076265 val loss 0.6929923378235748  train acc 93.21  val acc 77.78 lr: 9.986128001799077e-05\n",
      "Epoch 3: train loss 0.17366293044317335 val loss 0.7014485136752455  train acc 94.4  val acc 77.5 lr: 9.94459753267812e-05\n",
      "Epoch 4: train loss 0.1758162362234933 val loss 0.691564978315609  train acc 94.88  val acc 77.78 lr: 9.875664641789545e-05\n",
      "Epoch 5: train loss 0.18737369719005767 val loss 0.6760565268033158  train acc 93.81  val acc 77.22 lr: 9.779754323328191e-05\n",
      "Epoch 6: train loss 0.18902224586123512 val loss 0.6645275335140985  train acc 93.45  val acc 77.5 lr: 9.65745789630079e-05\n",
      "Epoch 7: train loss 0.18980705624534971 val loss 0.6522409633010536  train acc 92.14  val acc 78.06 lr: 9.509529358847654e-05\n",
      "Epoch 8: train loss 0.17867896670386904 val loss 0.6405936502650029  train acc 94.4  val acc 78.89 lr: 9.336880739593415e-05\n",
      "Epoch 9: train loss 0.18294417971656435 val loss 0.6384556814093596  train acc 92.74  val acc 78.89 lr: 9.140576474687264e-05\n",
      "Epoch 10: train loss 0.1844488053094773 val loss 0.6341335562052904  train acc 93.57  val acc 79.72 lr: 8.921826845200139e-05\n",
      "Epoch 11: train loss 0.17136041550409226 val loss 0.6332576937158466  train acc 94.29  val acc 79.72 lr: 8.681980515339464e-05\n",
      "Epoch 12: train loss 0.16733016967773437 val loss 0.6336323885226951  train acc 93.93  val acc 79.72 lr: 8.422516217485827e-05\n",
      "Epoch 13: train loss 0.17364936102004278 val loss 0.6302637804266106  train acc 93.21  val acc 79.72 lr: 8.14503363531613e-05\n",
      "Epoch 14: train loss 0.16209457034156435 val loss 0.6292744414184717  train acc 94.52  val acc 79.72 lr: 7.85124354122177e-05\n",
      "Epoch 15: train loss 0.18626136779785157 val loss 0.6281829827515986  train acc 94.17  val acc 79.72 lr: 7.54295724882796e-05\n",
      "Epoch 16: train loss 0.17147832598005022 val loss 0.6318706988582322  train acc 94.52  val acc 79.44 lr: 7.222075445642904e-05\n",
      "Epoch 17: train loss 0.16561170305524553 val loss 0.6328837937551245  train acc 95.0  val acc 79.44 lr: 6.890576474687263e-05\n",
      "Epoch 18: train loss 0.17407195681617374 val loss 0.6357897182931841  train acc 93.1  val acc 78.61 lr: 6.550504137351574e-05\n",
      "Epoch 19: train loss 0.15786668686639696 val loss 0.6363331277345752  train acc 93.45  val acc 78.61 lr: 6.203955092681039e-05\n",
      "Epoch 20: train loss 0.1687389373779297 val loss 0.6369869159246567  train acc 93.57  val acc 78.61 lr: 5.853065930775302e-05\n",
      "Epoch 21: train loss 0.16834776742117746 val loss 0.6389873958636801  train acc 94.17  val acc 78.89 lr: 5.4999999999999995e-05\n",
      "Epoch 22: train loss 0.1712614513578869 val loss 0.6419241181248342  train acc 93.21  val acc 78.61 lr: 5.146934069224698e-05\n",
      "Epoch 23: train loss 0.17159507388160342 val loss 0.6441379242227211  train acc 93.45  val acc 78.61 lr: 4.796044907318962e-05\n",
      "Epoch 24: train loss 0.17876710437593005 val loss 0.6440846582695725  train acc 92.98  val acc 78.33 lr: 4.4494958626484255e-05\n",
      "Epoch 25: train loss 0.1720970517113095 val loss 0.6452179850965738  train acc 94.64  val acc 78.33 lr: 4.109423525312736e-05\n",
      "Epoch 26: train loss 0.16941297621954055 val loss 0.6441421753919055  train acc 94.4  val acc 78.33 lr: 3.7779245543570955e-05\n",
      "Epoch 27: train loss 0.16444151742117746 val loss 0.6428956452481177  train acc 93.81  val acc 78.61 lr: 3.457042751172039e-05\n",
      "Epoch 28: train loss 0.17590435573032925 val loss 0.63990736854191  train acc 93.81  val acc 78.89 lr: 3.14875645877823e-05\n",
      "Epoch 29: train loss 0.15684994288853238 val loss 0.6396054031317956  train acc 95.0  val acc 78.89 lr: 2.854966364683871e-05\n",
      "Epoch 30: train loss 0.16138581775483632 val loss 0.6371237962522123  train acc 93.81  val acc 78.89 lr: 2.5774837825141737e-05\n",
      "Epoch 31: train loss 0.16294119698660714 val loss 0.6368700948200664  train acc 94.4  val acc 79.17 lr: 2.318019484660536e-05\n",
      "Epoch 32: train loss 0.16900055294945127 val loss 0.6367329133077319  train acc 94.29  val acc 78.61 lr: 2.0781731547998607e-05\n",
      "Epoch 33: train loss 0.18133819216773622 val loss 0.6364518081032757  train acc 93.45  val acc 78.33 lr: 1.8594235253127368e-05\n",
      "Epoch 34: train loss 0.17779831659226192 val loss 0.635091002362329  train acc 94.4  val acc 78.61 lr: 1.663119260406585e-05\n",
      "Epoch 35: train loss 0.16564187549409412 val loss 0.6336319640827279  train acc 94.29  val acc 78.61 lr: 1.4904706411523449e-05\n",
      "Epoch 36: train loss 0.15225019000825427 val loss 0.632723324126501  train acc 94.29  val acc 78.61 lr: 1.3425421036992097e-05\n",
      "Epoch 37: train loss 0.1686283656529018 val loss 0.6308540528760987  train acc 94.17  val acc 78.61 lr: 1.2202456766718091e-05\n",
      "Epoch 38: train loss 0.1646912347702753 val loss 0.6280992379044329  train acc 94.64  val acc 78.89 lr: 1.1243353582104556e-05\n",
      "Epoch 39: train loss 0.1698400406610398 val loss 0.6275353394064761  train acc 93.21  val acc 78.89 lr: 1.0554024673218807e-05\n",
      "Epoch 40: train loss 0.1653253646123977 val loss 0.6285286683568213  train acc 95.0  val acc 78.89 lr: 1.0138719982009242e-05\n",
      "Epoch 41: train loss 0.1723488580612909 val loss 0.627679280221298  train acc 93.93  val acc 78.89 lr: 1e-05\n",
      "Epoch 42: train loss 0.18302577790759858 val loss 0.6262635412974116  train acc 93.21  val acc 79.17 lr: 1.0138719982009242e-05\n",
      "Epoch 43: train loss 0.1695635296049572 val loss 0.6253144593854396  train acc 95.0  val acc 79.44 lr: 1.0554024673218818e-05\n",
      "Epoch 44: train loss 0.1607738494873047 val loss 0.6229408418284632  train acc 94.4  val acc 79.17 lr: 1.124335358210459e-05\n",
      "Epoch 45: train loss 0.16789917718796504 val loss 0.6229555251411777  train acc 93.57  val acc 79.44 lr: 1.220245676671815e-05\n",
      "Epoch 46: train loss 0.1533799852643694 val loss 0.6225750941566944  train acc 93.81  val acc 79.44 lr: 1.34254210369922e-05\n",
      "Epoch 47: train loss 0.17135957990373885 val loss 0.6223114577806718  train acc 93.93  val acc 79.44 lr: 1.49047064115236e-05\n",
      "Epoch 48: train loss 0.1590895516531808 val loss 0.6234744674145973  train acc 93.57  val acc 79.17 lr: 1.663119260406607e-05\n",
      "Epoch 49: train loss 0.1676715850830078 val loss 0.6228181060183929  train acc 93.57  val acc 79.44 lr: 1.8594235253127632e-05\n",
      "Epoch 50: train loss 0.16038560413178943 val loss 0.6224714321180019  train acc 94.52  val acc 79.44 lr: 2.0781731547998932e-05\n",
      "Epoch 51: train loss 0.16762137640090216 val loss 0.621595708110581  train acc 93.93  val acc 79.44 lr: 2.3180194846605774e-05\n",
      "Epoch 52: train loss 0.17984279450916107 val loss 0.6214134316526813  train acc 93.45  val acc 79.72 lr: 2.5774837825142218e-05\n",
      "Epoch 53: train loss 0.1760769798642113 val loss 0.6214887021687711  train acc 93.69  val acc 79.72 lr: 2.8549663646839293e-05\n",
      "Epoch 54: train loss 0.15688921610514323 val loss 0.622005693570793  train acc 94.64  val acc 79.72 lr: 3.1487564587783e-05\n",
      "Epoch 55: train loss 0.17668153671991257 val loss 0.6229801922878879  train acc 93.69  val acc 79.72 lr: 3.457042751172117e-05\n",
      "Epoch 56: train loss 0.16699053446451823 val loss 0.6215353839302945  train acc 94.4  val acc 80.0 lr: 3.777924554357186e-05\n",
      "Epoch 57: train loss 0.16970394679478237 val loss 0.6195751655194891  train acc 93.57  val acc 80.28 lr: 4.1094235253128355e-05\n",
      "Epoch 58: train loss 0.16204824901762463 val loss 0.6188500580079576  train acc 94.76  val acc 80.0 lr: 4.449495862648534e-05\n",
      "Epoch 59: train loss 0.16071711948939732 val loss 0.6212337908991483  train acc 95.12  val acc 80.28 lr: 4.796044907319082e-05\n",
      "Epoch 60: train loss 0.15332879566010973 val loss 0.6232054383926867  train acc 95.36  val acc 80.28 lr: 5.146934069224828e-05\n",
      "Epoch 61: train loss 0.16060731070382253 val loss 0.6240438052921503  train acc 94.76  val acc 80.28 lr: 5.500000000000143e-05\n",
      "Epoch 62: train loss 0.17086385091145834 val loss 0.6236495367401592  train acc 94.05  val acc 80.28 lr: 5.853065930775459e-05\n",
      "Epoch 63: train loss 0.16408595130557105 val loss 0.6251795916667043  train acc 93.45  val acc 80.28 lr: 6.203955092681206e-05\n",
      "Epoch 64: train loss 0.16529206775483632 val loss 0.6268367177749219  train acc 94.17  val acc 80.0 lr: 6.550504137351756e-05\n",
      "Epoch 65: train loss 0.16180723281133744 val loss 0.6302435956774907  train acc 94.76  val acc 79.44 lr: 6.890576474687454e-05\n",
      "Epoch 66: train loss 0.1751521519252232 val loss 0.6326705962935446  train acc 93.21  val acc 79.17 lr: 7.222075445643103e-05\n",
      "Epoch 67: train loss 0.16036204383486793 val loss 0.6345057931056785  train acc 93.69  val acc 78.89 lr: 7.542957248828171e-05\n",
      "Epoch 68: train loss 0.16474714733305432 val loss 0.6351387037524805  train acc 94.05  val acc 78.89 lr: 7.851243541221988e-05\n",
      "Epoch 69: train loss 0.16236099969773066 val loss 0.6421062299483645  train acc 94.05  val acc 78.61 lr: 8.145033635316357e-05\n",
      "Epoch 70: train loss 0.1541963123139881 val loss 0.6504205994621688  train acc 95.48  val acc 78.61 lr: 8.422516217486066e-05\n",
      "Epoch 71: train loss 0.15297477358863468 val loss 0.6621198391941865  train acc 94.4  val acc 77.5 lr: 8.68198051533971e-05\n",
      "Epoch 72: train loss 0.1637979053315662 val loss 0.682507390114933  train acc 94.17  val acc 77.22 lr: 8.921826845200394e-05\n",
      "Epoch 73: train loss 0.18239524478004093 val loss 0.6997211111036841  train acc 92.86  val acc 76.94 lr: 9.140576474687525e-05\n",
      "Epoch 74: train loss 0.1532005855015346 val loss 0.7091924564477128  train acc 95.24  val acc 76.94 lr: 9.336880739593682e-05\n",
      "Epoch 75: train loss 0.14846087864467075 val loss 0.7109729759725819  train acc 95.24  val acc 76.94 lr: 9.509529358847929e-05\n",
      "Epoch 76: train loss 0.1602427528018043 val loss 0.7075295671653831  train acc 94.29  val acc 76.67 lr: 9.657457896301067e-05\n",
      "Epoch 77: train loss 0.16102654593331472 val loss 0.6997463187803943  train acc 94.29  val acc 76.94 lr: 9.779754323328473e-05\n",
      "Epoch 78: train loss 0.15573588779994418 val loss 0.6989972474243492  train acc 94.64  val acc 77.22 lr: 9.875664641789831e-05\n",
      "Epoch 79: train loss 0.15772445315406436 val loss 0.6869187772377598  train acc 94.05  val acc 77.5 lr: 9.944597532678408e-05\n",
      "Epoch 80: train loss 0.15660511198497953 val loss 0.6740316174858921  train acc 94.52  val acc 77.78 lr: 9.986128001799367e-05\n",
      "Epoch 81: train loss 0.15879244123186384 val loss 0.6590619203721114  train acc 93.57  val acc 78.06 lr: 0.00010000000000000292\n",
      "Epoch 82: train loss 0.15209757486979167 val loss 0.6439716950197811  train acc 93.93  val acc 79.17 lr: 9.986128001799368e-05\n",
      "Epoch 83: train loss 0.1567798796154204 val loss 0.6336519326074647  train acc 94.52  val acc 80.56 lr: 9.94459753267841e-05\n",
      "Epoch 84: train loss 0.15976006644112722 val loss 0.6283668283332765  train acc 93.69  val acc 81.11 lr: 9.875664641789834e-05\n",
      "Epoch 85: train loss 0.1479308355422247 val loss 0.6280064691927356  train acc 94.52  val acc 81.11 lr: 9.779754323328477e-05\n",
      "Epoch 86: train loss 0.15722794305710566 val loss 0.6277280752947068  train acc 94.4  val acc 81.39 lr: 9.657457896301071e-05\n",
      "Epoch 87: train loss 0.1463042667933873 val loss 0.6279573358937612  train acc 95.0  val acc 81.39 lr: 9.509529358847932e-05\n",
      "Epoch 88: train loss 0.14311205546061198 val loss 0.6289270106782037  train acc 95.95  val acc 81.11 lr: 9.336880739593687e-05\n",
      "Epoch 89: train loss 0.15751524425688243 val loss 0.6287211716003779  train acc 94.17  val acc 81.11 lr: 9.14057647468753e-05\n",
      "Epoch 90: train loss 0.15407609485444568 val loss 0.6302539694457008  train acc 94.76  val acc 80.56 lr: 8.921826845200395e-05\n",
      "Epoch 91: train loss 0.14881533668154762 val loss 0.6320601085861325  train acc 94.29  val acc 80.28 lr: 8.681980515339713e-05\n",
      "Epoch 92: train loss 0.15327420915876117 val loss 0.6333329021163536  train acc 95.36  val acc 80.83 lr: 8.42251621748607e-05\n",
      "Epoch 93: train loss 0.16115352085658483 val loss 0.6402073224978191  train acc 94.76  val acc 79.44 lr: 8.145033635316361e-05\n",
      "Epoch 94: train loss 0.15508328392392112 val loss 0.6514552884991492  train acc 95.12  val acc 78.33 lr: 7.851243541221995e-05\n",
      "Epoch 95: train loss 0.15350837707519532 val loss 0.6606941787698095  train acc 94.76  val acc 77.78 lr: 7.542957248828171e-05\n",
      "Epoch 96: train loss 0.14994501386369977 val loss 0.6667927520011353  train acc 94.76  val acc 77.78 lr: 7.222075445643105e-05\n",
      "Epoch 97: train loss 0.14260464622860863 val loss 0.6701491587282722  train acc 95.0  val acc 77.78 lr: 6.890576474687456e-05\n",
      "Epoch 98: train loss 0.14269908723377045 val loss 0.6738524055809105  train acc 95.0  val acc 77.78 lr: 6.550504137351759e-05\n",
      "Epoch 99: train loss 0.1480321793329148 val loss 0.6704625533617332  train acc 94.64  val acc 77.78 lr: 6.203955092681214e-05\n",
      "Epoch 100: train loss 0.1478014628092448 val loss 0.6679222326956942  train acc 94.76  val acc 77.78 lr: 5.85306593077546e-05\n",
      "Epoch 101: train loss 0.14013914380754744 val loss 0.6625618214227365  train acc 95.71  val acc 78.06 lr: 5.500000000000148e-05\n",
      "Epoch 102: train loss 0.15751248314267113 val loss 0.6581027760983823  train acc 94.29  val acc 78.33 lr: 5.146934069224832e-05\n",
      "Epoch 103: train loss 0.1433620362054734 val loss 0.6547669750567559  train acc 95.0  val acc 78.61 lr: 4.796044907319091e-05\n",
      "Epoch 104: train loss 0.14662025088355654 val loss 0.6522698973266327  train acc 94.4  val acc 78.61 lr: 4.449495862648542e-05\n",
      "Epoch 105: train loss 0.14612530299595425 val loss 0.6525738173502393  train acc 94.76  val acc 78.61 lr: 4.1094235253128396e-05\n",
      "Epoch 106: train loss 0.14186737423851375 val loss 0.6530733614466296  train acc 94.88  val acc 78.61 lr: 3.777924554357187e-05\n",
      "Epoch 107: train loss 0.1465671630132766 val loss 0.6531319310695471  train acc 95.24  val acc 78.61 lr: 3.457042751172118e-05\n",
      "Epoch 108: train loss 0.14883583613804408 val loss 0.6540274351987487  train acc 94.29  val acc 78.61 lr: 3.148756458778304e-05\n",
      "Epoch 109: train loss 0.13963904607863653 val loss 0.6543884118392349  train acc 95.95  val acc 78.61 lr: 2.8549663646839333e-05\n",
      "Epoch 110: train loss 0.15778530665806362 val loss 0.6528637448104794  train acc 94.05  val acc 78.61 lr: 2.5774837825142313e-05\n",
      "Epoch 111: train loss 0.1586997804187593 val loss 0.6515769184890154  train acc 94.4  val acc 78.33 lr: 2.318019484660578e-05\n",
      "Epoch 112: train loss 0.1511956532796224 val loss 0.653146887688971  train acc 95.0  val acc 78.06 lr: 2.0781731547998993e-05\n",
      "Epoch 113: train loss 0.1477146511986142 val loss 0.6558691764549593  train acc 94.52  val acc 78.06 lr: 1.859423525312766e-05\n",
      "Epoch 114: train loss 0.14965221768333797 val loss 0.6563646933832145  train acc 94.64  val acc 78.06 lr: 1.663119260406607e-05\n",
      "Epoch 115: train loss 0.14887244814918155 val loss 0.6555180636649396  train acc 95.0  val acc 78.06 lr: 1.4904706411523634e-05\n",
      "Epoch 116: train loss 0.14908449082147507 val loss 0.653846592510572  train acc 94.64  val acc 78.33 lr: 1.3425421036992191e-05\n",
      "Epoch 117: train loss 0.15967225574311755 val loss 0.6529887223962949  train acc 94.29  val acc 78.33 lr: 1.2202456766718171e-05\n",
      "Epoch 118: train loss 0.15632002694266184 val loss 0.6534341969162976  train acc 94.52  val acc 78.33 lr: 1.1243353582104592e-05\n",
      "Epoch 119: train loss 0.14607147035144624 val loss 0.6516983221028024  train acc 95.0  val acc 78.61 lr: 1.0554024673218829e-05\n",
      "Epoch 120: train loss 0.14368360610235306 val loss 0.6495034285511613  train acc 95.36  val acc 79.17 lr: 1.0138719982009247e-05\n",
      "Epoch 121: train loss 0.14746471586681548 val loss 0.6473598434030982  train acc 95.6  val acc 79.17 lr: 1e-05\n",
      "Epoch 122: train loss 0.15711353846958706 val loss 0.6483154448709643  train acc 94.05  val acc 79.17 lr: 1.0138719982009242e-05\n",
      "Epoch 123: train loss 0.1349965595063709 val loss 0.6488654737314603  train acc 95.48  val acc 79.17 lr: 1.0554024673218803e-05\n",
      "Epoch 124: train loss 0.1569683074951172 val loss 0.6498031246043408  train acc 94.05  val acc 79.17 lr: 1.124335358210454e-05\n",
      "Epoch 125: train loss 0.14487735203334262 val loss 0.6501134435890306  train acc 94.76  val acc 79.17 lr: 1.2202456766718076e-05\n",
      "Epoch 126: train loss 0.1499778293427967 val loss 0.6496279069271809  train acc 95.12  val acc 79.17 lr: 1.342542103699205e-05\n",
      "Epoch 127: train loss 0.14856929779052735 val loss 0.6508972606594312  train acc 94.76  val acc 79.17 lr: 1.4904706411523432e-05\n",
      "Epoch 128: train loss 0.12723855518159413 val loss 0.6518605552323629  train acc 96.07  val acc 78.61 lr: 1.66311926040658e-05\n",
      "Epoch 129: train loss 0.14445225851876395 val loss 0.6497902939117438  train acc 95.48  val acc 79.17 lr: 1.8594235253127314e-05\n",
      "Epoch 130: train loss 0.14299846830822172 val loss 0.6501725223788478  train acc 94.88  val acc 78.89 lr: 2.0781731547998556e-05\n",
      "Epoch 131: train loss 0.14060153052920388 val loss 0.6521074842509844  train acc 95.36  val acc 78.33 lr: 2.3180194846605256e-05\n",
      "Epoch 132: train loss 0.11677467709495908 val loss 0.6551308262232431  train acc 96.9  val acc 78.33 lr: 2.577483782514169e-05\n",
      "Epoch 133: train loss 0.13193885258265903 val loss 0.6585655564699847  train acc 95.12  val acc 78.33 lr: 2.854966364683861e-05\n",
      "Epoch 134: train loss 0.14808959052676246 val loss 0.659769964836332  train acc 94.64  val acc 78.33 lr: 3.1487564587782205e-05\n",
      "Epoch 135: train loss 0.1551634288969494 val loss 0.6597282965784375  train acc 94.52  val acc 78.33 lr: 3.457042751172023e-05\n",
      "Epoch 136: train loss 0.14689486367361887 val loss 0.6575794266119471  train acc 94.4  val acc 78.33 lr: 3.77792455435708e-05\n",
      "Epoch 137: train loss 0.1528583708263579 val loss 0.6564764490507297  train acc 94.88  val acc 78.33 lr: 4.1094235253127203e-05\n",
      "Epoch 138: train loss 0.13765682038806734 val loss 0.6534444845747516  train acc 95.0  val acc 78.33 lr: 4.44949586264841e-05\n",
      "Epoch 139: train loss 0.16005961100260416 val loss 0.6498929403514148  train acc 93.45  val acc 79.17 lr: 4.7960449073189464e-05\n",
      "Epoch 140: train loss 0.1467805680774507 val loss 0.6488928151038856  train acc 94.76  val acc 79.17 lr: 5.146934069224676e-05\n",
      "Epoch 141: train loss 0.1336892355056036 val loss 0.6470920165990299  train acc 95.83  val acc 79.44 lr: 5.499999999999979e-05\n",
      "Epoch 142: train loss 0.1454485847836449 val loss 0.6433926398926673  train acc 95.36  val acc 79.44 lr: 5.8530659307752745e-05\n",
      "Epoch 143: train loss 0.1361561548142206 val loss 0.643082005474774  train acc 95.83  val acc 79.72 lr: 6.203955092681019e-05\n",
      "Epoch 144: train loss 0.146735836210705 val loss 0.641765781013762  train acc 94.52  val acc 80.0 lr: 6.550504137351547e-05\n",
      "Epoch 145: train loss 0.13541983649844214 val loss 0.6404259315604406  train acc 95.36  val acc 79.72 lr: 6.890576474687237e-05\n",
      "Epoch 146: train loss 0.15261303129650297 val loss 0.6408454097864323  train acc 94.76  val acc 80.28 lr: 7.222075445642879e-05\n",
      "Epoch 147: train loss 0.1430293946039109 val loss 0.6435211965562329  train acc 95.71  val acc 80.0 lr: 7.542957248827929e-05\n",
      "Epoch 148: train loss 0.1460154578799293 val loss 0.6465629608365008  train acc 95.12  val acc 79.72 lr: 7.851243541221745e-05\n",
      "Epoch 149: train loss 0.14451197669619606 val loss 0.6527215769616854  train acc 94.88  val acc 80.0 lr: 8.145033635316098e-05\n",
      "Epoch 150: train loss 0.1367702393304734 val loss 0.6615106748968874  train acc 95.36  val acc 78.61 lr: 8.422516217485796e-05\n",
      "Epoch 151: train loss 0.13181165967668806 val loss 0.6699076946924862  train acc 95.24  val acc 78.33 lr: 8.681980515339428e-05\n",
      "Epoch 152: train loss 0.13989117940266926 val loss 0.674245143882599  train acc 94.64  val acc 78.06 lr: 8.921826845200104e-05\n",
      "Epoch 153: train loss 0.14567025502522787 val loss 0.6752579530288882  train acc 94.76  val acc 78.06 lr: 9.140576474687227e-05\n",
      "Epoch 154: train loss 0.13587596530006046 val loss 0.6743307698082289  train acc 95.71  val acc 78.06 lr: 9.336880739593379e-05\n",
      "Epoch 155: train loss 0.13162799108596074 val loss 0.6743620211741624  train acc 95.83  val acc 78.06 lr: 9.509529358847619e-05\n",
      "Epoch 156: train loss 0.1358228047688802 val loss 0.676522098583474  train acc 95.0  val acc 78.06 lr: 9.657457896300751e-05\n",
      "Epoch 157: train loss 0.1374766395205543 val loss 0.6784021276662094  train acc 95.24  val acc 78.33 lr: 9.779754323328152e-05\n",
      "Epoch 158: train loss 0.14265943254743305 val loss 0.6838204423904861  train acc 95.95  val acc 78.33 lr: 9.875664641789504e-05\n",
      "Epoch 159: train loss 0.14305018470400854 val loss 0.6908359343148723  train acc 94.76  val acc 78.06 lr: 9.94459753267808e-05\n",
      "Epoch 160: train loss 0.14142428806849888 val loss 0.6956359402415733  train acc 95.48  val acc 77.78 lr: 9.986128001799035e-05\n",
      "Epoch 161: train loss 0.1333793186006092 val loss 0.6946881787719476  train acc 96.07  val acc 77.78 lr: 9.999999999999958e-05\n",
      "Epoch 162: train loss 0.14435174124581474 val loss 0.685162652915934  train acc 95.24  val acc 77.78 lr: 9.986128001799035e-05\n",
      "Epoch 163: train loss 0.12329287756057013 val loss 0.6761099634292523  train acc 95.83  val acc 77.78 lr: 9.94459753267808e-05\n",
      "Epoch 164: train loss 0.14140136355445498 val loss 0.6673799083375561  train acc 95.0  val acc 78.33 lr: 9.875664641789504e-05\n",
      "Epoch 165: train loss 0.12834972199939546 val loss 0.6607380667241589  train acc 95.83  val acc 78.89 lr: 9.779754323328153e-05\n",
      "Epoch 166: train loss 0.1366764704386393 val loss 0.6627173058755224  train acc 95.71  val acc 78.33 lr: 9.657457896300753e-05\n",
      "Epoch 167: train loss 0.13283943902878534 val loss 0.6652395258377141  train acc 95.24  val acc 77.78 lr: 9.509529358847616e-05\n",
      "Epoch 168: train loss 0.13582670120965867 val loss 0.6615361576788966  train acc 95.36  val acc 78.06 lr: 9.336880739593374e-05\n",
      "Epoch 169: train loss 0.13039410909016927 val loss 0.6603954570312658  train acc 95.48  val acc 78.61 lr: 9.140576474687227e-05\n",
      "Epoch 170: train loss 0.1277127947126116 val loss 0.6587471210725605  train acc 96.07  val acc 78.61 lr: 8.921826845200102e-05\n",
      "Epoch 171: train loss 0.1335473106020973 val loss 0.6539097883928829  train acc 95.48  val acc 78.89 lr: 8.681980515339427e-05\n",
      "Epoch 172: train loss 0.12869370778401693 val loss 0.6512511745373502  train acc 96.07  val acc 79.44 lr: 8.422516217485795e-05\n",
      "Epoch 173: train loss 0.1271794637044271 val loss 0.6477048389690941  train acc 96.31  val acc 79.17 lr: 8.145033635316098e-05\n",
      "Epoch 174: train loss 0.13621397472563243 val loss 0.6421100317794279  train acc 95.12  val acc 80.28 lr: 7.851243541221745e-05\n",
      "Epoch 175: train loss 0.12990742637997582 val loss 0.6406207752058833  train acc 95.95  val acc 80.0 lr: 7.542957248827936e-05\n",
      "Epoch 176: train loss 0.12067830221993582 val loss 0.6392752080875385  train acc 95.95  val acc 80.28 lr: 7.222075445642879e-05\n",
      "Epoch 177: train loss 0.13493795849028087 val loss 0.6389100085886087  train acc 95.83  val acc 80.0 lr: 6.890576474687247e-05\n",
      "Epoch 178: train loss 0.12212220146542503 val loss 0.6399131480086153  train acc 96.67  val acc 80.0 lr: 6.55050413735155e-05\n",
      "Epoch 179: train loss 0.1332737604777018 val loss 0.6415492720299654  train acc 95.48  val acc 80.0 lr: 6.203955092681015e-05\n",
      "Epoch 180: train loss 0.1350637708391462 val loss 0.6416812089391739  train acc 95.48  val acc 80.28 lr: 5.853065930775278e-05\n",
      "Epoch 181: train loss 0.1177102315993536 val loss 0.6425637829259279  train acc 96.9  val acc 80.28 lr: 5.4999999999999826e-05\n",
      "Epoch 182: train loss 0.12679445175897508 val loss 0.643583846470235  train acc 95.24  val acc 80.28 lr: 5.1469340692246805e-05\n",
      "Epoch 183: train loss 0.11427489689418248 val loss 0.6438438102050356  train acc 96.43  val acc 80.28 lr: 4.796044907318952e-05\n",
      "Epoch 184: train loss 0.1288991201491583 val loss 0.6442192709162029  train acc 95.6  val acc 80.28 lr: 4.4494958626484154e-05\n",
      "Epoch 185: train loss 0.1296366192045666 val loss 0.6452721574809165  train acc 96.07  val acc 80.28 lr: 4.109423525312726e-05\n",
      "Epoch 186: train loss 0.12711723872593472 val loss 0.6469911304749849  train acc 95.71  val acc 80.28 lr: 3.777924554357092e-05\n",
      "Epoch 187: train loss 0.11534708113897414 val loss 0.6488922003550145  train acc 96.31  val acc 80.28 lr: 3.4570427511720356e-05\n",
      "Epoch 188: train loss 0.13054179237002417 val loss 0.648582740147339  train acc 95.6  val acc 79.72 lr: 3.148756458778219e-05\n",
      "Epoch 189: train loss 0.1298685800461542 val loss 0.6520714773532869  train acc 95.6  val acc 79.72 lr: 2.8549663646838595e-05\n",
      "Epoch 190: train loss 0.1305853979928153 val loss 0.6541314630724985  train acc 95.95  val acc 79.72 lr: 2.5774837825141683e-05\n",
      "Epoch 191: train loss 0.12885264442080543 val loss 0.6561478562842293  train acc 95.6  val acc 79.72 lr: 2.3180194846605303e-05\n",
      "Epoch 192: train loss 0.12761604672386534 val loss 0.6580653916476911  train acc 95.6  val acc 79.72 lr: 2.07817315479986e-05\n",
      "Epoch 193: train loss 0.11878309704008556 val loss 0.6600926383944805  train acc 96.31  val acc 79.44 lr: 1.8594235253127348e-05\n",
      "Epoch 194: train loss 0.13215544564383372 val loss 0.6620132207451714  train acc 95.71  val acc 79.44 lr: 1.6631192604065828e-05\n",
      "Epoch 195: train loss 0.12578697204589845 val loss 0.6633651206981663  train acc 95.83  val acc 79.44 lr: 1.4904706411523457e-05\n",
      "Epoch 196: train loss 0.1350014913649786 val loss 0.6637251939480965  train acc 95.36  val acc 78.89 lr: 1.34254210369921e-05\n",
      "Epoch 197: train loss 0.12864286331903366 val loss 0.6661744566961377  train acc 94.76  val acc 78.61 lr: 1.2202456766718113e-05\n",
      "Epoch 198: train loss 0.11914252326602028 val loss 0.6657183911146787  train acc 95.95  val acc 78.89 lr: 1.1243353582104571e-05\n",
      "Epoch 199: train loss 0.13928558712913877 val loss 0.6647517249462355  train acc 95.36  val acc 78.61 lr: 1.05540246732188e-05\n",
      "Epoch 200: train loss 0.13879004887172153 val loss 0.6661211063338666  train acc 95.0  val acc 78.89 lr: 1.0138719982009237e-05\n",
      "Epoch 201: train loss 0.14941949390229725 val loss 0.6659824128496328  train acc 94.76  val acc 79.17 lr: 1e-05\n",
      "Epoch 202: train loss 0.13445945921398345 val loss 0.668348226737836  train acc 95.12  val acc 79.17 lr: 1.0138719982009242e-05\n",
      "Epoch 203: train loss 0.1139366694859096 val loss 0.6685436979627181  train acc 96.55  val acc 79.44 lr: 1.0554024673218803e-05\n",
      "Epoch 204: train loss 0.14012777237665086 val loss 0.6687916730376163  train acc 95.48  val acc 79.44 lr: 1.1243353582104534e-05\n",
      "Epoch 205: train loss 0.13646220252627417 val loss 0.6702964923362474  train acc 95.0  val acc 79.44 lr: 1.2202456766718046e-05\n",
      "Epoch 206: train loss 0.124766358875093 val loss 0.6720337827709348  train acc 96.19  val acc 79.17 lr: 1.3425421036992046e-05\n",
      "Epoch 207: train loss 0.1481870832897368 val loss 0.6707992954480384  train acc 94.4  val acc 78.89 lr: 1.4904706411523391e-05\n",
      "Epoch 208: train loss 0.13258419036865235 val loss 0.6722184036683997  train acc 95.24  val acc 78.61 lr: 1.6631192604065794e-05\n",
      "Epoch 209: train loss 0.14500226520356677 val loss 0.6731508402203613  train acc 94.64  val acc 78.61 lr: 1.8594235253127307e-05\n",
      "Epoch 210: train loss 0.1350580033801851 val loss 0.6745558202787909  train acc 95.36  val acc 78.61 lr: 2.0781731547998553e-05\n",
      "Epoch 211: train loss 0.1288341794695173 val loss 0.6752005928466496  train acc 95.83  val acc 78.61 lr: 2.3180194846605306e-05\n",
      "Epoch 212: train loss 0.1330772490728469 val loss 0.6764545947483738  train acc 95.83  val acc 78.33 lr: 2.5774837825141683e-05\n",
      "Epoch 213: train loss 0.13597531091599238 val loss 0.6775497230880063  train acc 95.6  val acc 78.33 lr: 2.8549663646838656e-05\n",
      "Epoch 214: train loss 0.11940586453392392 val loss 0.6755570629377186  train acc 96.79  val acc 78.06 lr: 3.1487564587782117e-05\n",
      "Epoch 215: train loss 0.1127007075718471 val loss 0.6735831027381263  train acc 95.95  val acc 78.61 lr: 3.4570427511720214e-05\n",
      "Epoch 216: train loss 0.12509307861328126 val loss 0.6715042572330905  train acc 95.36  val acc 78.89 lr: 3.7779245543570785e-05\n",
      "Epoch 217: train loss 0.13146740141369048 val loss 0.6675887195968503  train acc 96.31  val acc 79.17 lr: 4.109423525312719e-05\n",
      "Epoch 218: train loss 0.11813332693917411 val loss 0.6664534566330262  train acc 96.31  val acc 78.89 lr: 4.4494958626484086e-05\n",
      "Epoch 219: train loss 0.13129002707345144 val loss 0.6651813907392311  train acc 95.12  val acc 79.44 lr: 4.796044907318929e-05\n",
      "Epoch 220: train loss 0.1244467508225214 val loss 0.6616769711985145  train acc 94.76  val acc 80.28 lr: 5.146934069224682e-05\n",
      "Epoch 221: train loss 0.12795089540027438 val loss 0.6601142891766781  train acc 95.48  val acc 80.0 lr: 5.4999999999999846e-05\n",
      "Epoch 222: train loss 0.12892556871686664 val loss 0.6597082062196964  train acc 95.36  val acc 80.0 lr: 5.853065930775288e-05\n",
      "Epoch 223: train loss 0.13399197714669364 val loss 0.6602774837539636  train acc 95.12  val acc 80.28 lr: 6.203955092681009e-05\n",
      "Epoch 224: train loss 0.12644623347691128 val loss 0.6618423279724077  train acc 96.55  val acc 80.0 lr: 6.550504137351544e-05\n",
      "Epoch 225: train loss 0.12379440125964937 val loss 0.6662478247304544  train acc 95.6  val acc 78.89 lr: 6.890576474687236e-05\n",
      "Epoch 226: train loss 0.11874071756998698 val loss 0.6715823479081668  train acc 96.31  val acc 78.33 lr: 7.222075445642878e-05\n",
      "Epoch 227: train loss 0.13799155099051338 val loss 0.6834116796013819  train acc 95.12  val acc 78.06 lr: 7.542957248827934e-05\n",
      "Epoch 228: train loss 0.1134066627139137 val loss 0.6928272909795683  train acc 96.79  val acc 78.33 lr: 7.851243541221729e-05\n",
      "Epoch 229: train loss 0.12609346480596634 val loss 0.7022985540785792  train acc 95.36  val acc 77.5 lr: 8.145033635316089e-05\n",
      "Epoch 230: train loss 0.12750876290457588 val loss 0.7104230038701985  train acc 95.83  val acc 77.5 lr: 8.422516217485787e-05\n",
      "Epoch 231: train loss 0.11806600661504836 val loss 0.7129283694503761  train acc 96.31  val acc 77.5 lr: 8.681980515339435e-05\n",
      "Epoch 232: train loss 0.12167817978631883 val loss 0.71710470956867  train acc 95.48  val acc 77.22 lr: 8.921826845200101e-05\n",
      "Epoch 233: train loss 0.12341686430431548 val loss 0.7221980544898887  train acc 96.43  val acc 76.67 lr: 9.140576474687227e-05\n",
      "Epoch 234: train loss 0.12062515077136812 val loss 0.7278988592256329  train acc 95.6  val acc 76.39 lr: 9.336880739593379e-05\n",
      "Epoch 235: train loss 0.12542523883637927 val loss 0.7331329750362546  train acc 95.71  val acc 76.67 lr: 9.509529358847619e-05\n",
      "Epoch 236: train loss 0.12284676688058035 val loss 0.7374609614711229  train acc 96.31  val acc 76.67 lr: 9.657457896300755e-05\n",
      "Epoch 237: train loss 0.11898226056780134 val loss 0.7401735100142761  train acc 95.71  val acc 76.67 lr: 9.779754323328152e-05\n",
      "Epoch 238: train loss 0.12265523274739583 val loss 0.7377839256683365  train acc 95.71  val acc 76.67 lr: 9.875664641789507e-05\n",
      "Epoch 239: train loss 0.12871119181315105 val loss 0.717453887340887  train acc 95.24  val acc 76.94 lr: 9.944597532678083e-05\n",
      "Epoch 240: train loss 0.12415865943545387 val loss 0.7015776759694821  train acc 95.36  val acc 77.78 lr: 9.986128001799039e-05\n",
      "Epoch 241: train loss 0.13145943596249535 val loss 0.6914050332957856  train acc 96.07  val acc 78.33 lr: 9.999999999999963e-05\n",
      "Epoch 242: train loss 0.1111080078851609 val loss 0.6805121864298099  train acc 96.19  val acc 78.33 lr: 9.986128001799039e-05\n",
      "Epoch 243: train loss 0.1306875501360212 val loss 0.6679714029677648  train acc 95.48  val acc 79.44 lr: 9.944597532678083e-05\n",
      "Epoch 244: train loss 0.13013824281238373 val loss 0.6599665246611415  train acc 95.48  val acc 80.0 lr: 9.875664641789507e-05\n",
      "Epoch 245: train loss 0.11916849953787667 val loss 0.6539765117579953  train acc 96.67  val acc 80.83 lr: 9.779754323328153e-05\n",
      "Epoch 246: train loss 0.12282430557977586 val loss 0.6506341320105934  train acc 95.36  val acc 80.56 lr: 9.657457896300757e-05\n",
      "Epoch 247: train loss 0.11973863329206194 val loss 0.6494453567310864  train acc 95.71  val acc 81.67 lr: 9.509529358847623e-05\n",
      "Epoch 248: train loss 0.13328401474725632 val loss 0.6487795388807711  train acc 95.12  val acc 81.39 lr: 9.336880739593382e-05\n",
      "Epoch 249: train loss 0.10489880698067801 val loss 0.649939567300983  train acc 96.43  val acc 80.56 lr: 9.140576474687231e-05\n",
      "Epoch 250: train loss 0.12105738321940104 val loss 0.6512643571753683  train acc 95.36  val acc 80.56 lr: 8.921826845200107e-05\n",
      "Epoch 251: train loss 0.09931635175432478 val loss 0.6553197440484587  train acc 97.02  val acc 80.28 lr: 8.681980515339442e-05\n",
      "Epoch 252: train loss 0.12062457856677827 val loss 0.6600788051724223  train acc 95.6  val acc 80.0 lr: 8.422516217485792e-05\n",
      "Epoch 253: train loss 0.11750398363385882 val loss 0.666132123260035  train acc 95.6  val acc 79.44 lr: 8.145033635316096e-05\n",
      "Epoch 254: train loss 0.1209525153750465 val loss 0.6730807338005433  train acc 95.48  val acc 78.61 lr: 7.851243541221735e-05\n",
      "Epoch 255: train loss 0.11568825131370908 val loss 0.6875672528273447  train acc 95.71  val acc 77.78 lr: 7.54295724882794e-05\n",
      "Epoch 256: train loss 0.10807856605166481 val loss 0.7060551488460743  train acc 96.07  val acc 78.06 lr: 7.222075445642883e-05\n",
      "Epoch 257: train loss 0.12042835780552455 val loss 0.7173586348422163  train acc 96.07  val acc 78.06 lr: 6.890576474687243e-05\n",
      "Epoch 258: train loss 0.11830979301815941 val loss 0.7264187480843122  train acc 95.71  val acc 77.22 lr: 6.550504137351554e-05\n",
      "Epoch 259: train loss 0.1101075671968006 val loss 0.7293375392858802  train acc 96.19  val acc 76.94 lr: 6.203955092681017e-05\n",
      "Epoch 260: train loss 0.12785467420305524 val loss 0.7313267079911479  train acc 96.31  val acc 76.94 lr: 5.853065930775296e-05\n",
      "Epoch 261: train loss 0.12794272104899088 val loss 0.7241624738898584  train acc 95.95  val acc 76.39 lr: 5.499999999999994e-05\n",
      "Epoch 262: train loss 0.12668470655168806 val loss 0.7205685966220999  train acc 95.71  val acc 76.94 lr: 5.146934069224691e-05\n",
      "Epoch 263: train loss 0.1095745359148298 val loss 0.7106855672344777  train acc 96.67  val acc 78.06 lr: 4.7960449073189376e-05\n",
      "Epoch 264: train loss 0.11846988314674015 val loss 0.7015307638489461  train acc 95.36  val acc 78.06 lr: 4.4494958626484174e-05\n",
      "Epoch 265: train loss 0.10975065685453869 val loss 0.69431256903446  train acc 96.19  val acc 78.06 lr: 4.109423525312728e-05\n",
      "Epoch 266: train loss 0.11990526290166946 val loss 0.6929787500738374  train acc 95.71  val acc 78.06 lr: 3.777924554357087e-05\n",
      "Epoch 267: train loss 0.12126108805338541 val loss 0.6887448005114221  train acc 96.07  val acc 78.33 lr: 3.4570427511720295e-05\n",
      "Epoch 268: train loss 0.10266383943103609 val loss 0.6829267760166348  train acc 97.26  val acc 78.61 lr: 3.14875645877822e-05\n",
      "Epoch 269: train loss 0.1262322017124721 val loss 0.6777041614443304  train acc 95.48  val acc 78.89 lr: 2.8549663646838737e-05\n",
      "Epoch 270: train loss 0.12053312574114118 val loss 0.675336470016422  train acc 96.55  val acc 78.89 lr: 2.5774837825141757e-05\n",
      "Epoch 271: train loss 0.11188443501790364 val loss 0.6752289945007968  train acc 96.55  val acc 78.89 lr: 2.318019484660537e-05\n",
      "Epoch 272: train loss 0.1190441404070173 val loss 0.674219582979489  train acc 95.71  val acc 79.44 lr: 2.0781731547998607e-05\n",
      "Epoch 273: train loss 0.11755223047165643 val loss 0.6721565429823404  train acc 96.07  val acc 79.44 lr: 1.859423525312736e-05\n",
      "Epoch 274: train loss 0.1217266355242048 val loss 0.670323006669089  train acc 95.95  val acc 79.72 lr: 1.6631192604065835e-05\n",
      "Epoch 275: train loss 0.11316633678617932 val loss 0.6693532987754621  train acc 96.07  val acc 79.72 lr: 1.4904706411523429e-05\n",
      "Epoch 276: train loss 0.12384570893787203 val loss 0.668023433334941  train acc 96.31  val acc 79.72 lr: 1.3425421036992076e-05\n",
      "Epoch 277: train loss 0.12090156191871279 val loss 0.6697709737832092  train acc 96.19  val acc 79.72 lr: 1.220245676671807e-05\n",
      "Epoch 278: train loss 0.11060196104503814 val loss 0.669321080180335  train acc 96.67  val acc 79.72 lr: 1.1243353582104571e-05\n",
      "Epoch 279: train loss 0.12130653744652158 val loss 0.669229221863853  train acc 95.48  val acc 79.72 lr: 1.0554024673218815e-05\n",
      "Epoch 280: train loss 0.10050320398239862 val loss 0.66857865356021  train acc 96.79  val acc 79.72 lr: 1.0138719982009247e-05\n",
      "Epoch 281: train loss 0.11353436424618675 val loss 0.6696756173084452  train acc 96.07  val acc 79.72 lr: 1e-05\n",
      "Epoch 282: train loss 0.11500366755894252 val loss 0.6689708825484078  train acc 95.83  val acc 79.72 lr: 1.0138719982009242e-05\n",
      "Epoch 283: train loss 0.11418025607154483 val loss 0.669186793976318  train acc 96.55  val acc 79.72 lr: 1.0554024673218774e-05\n",
      "Epoch 284: train loss 0.12569454738071986 val loss 0.6687224769110678  train acc 95.48  val acc 79.72 lr: 1.1243353582104556e-05\n",
      "Epoch 285: train loss 0.11191681453159877 val loss 0.6699842993495201  train acc 95.95  val acc 79.72 lr: 1.2202456766718093e-05\n",
      "Epoch 286: train loss 0.10399575006394159 val loss 0.6710050095287752  train acc 96.9  val acc 79.72 lr: 1.3425421036992103e-05\n",
      "Epoch 287: train loss 0.10906289418538412 val loss 0.6719179072809831  train acc 96.19  val acc 79.72 lr: 1.4904706411523391e-05\n",
      "Epoch 288: train loss 0.11580506279355003 val loss 0.6724571971087312  train acc 96.07  val acc 79.44 lr: 1.663119260406579e-05\n",
      "Epoch 289: train loss 0.11062620253789993 val loss 0.6737283846095471  train acc 96.79  val acc 79.44 lr: 1.859423525312731e-05\n",
      "Epoch 290: train loss 0.11647789364769345 val loss 0.6736143834462835  train acc 95.95  val acc 79.44 lr: 2.0781731547998553e-05\n",
      "Epoch 291: train loss 0.11700743720644996 val loss 0.6733062555592781  train acc 95.6  val acc 79.72 lr: 2.318019484660531e-05\n",
      "Epoch 292: train loss 0.10384092785063244 val loss 0.675025001638821  train acc 96.67  val acc 79.17 lr: 2.5774837825141574e-05\n",
      "Epoch 293: train loss 0.11495590209960938 val loss 0.6772247120794759  train acc 96.19  val acc 79.17 lr: 2.8549663646838547e-05\n",
      "Epoch 294: train loss 0.11699261438278925 val loss 0.6784989336235128  train acc 95.48  val acc 78.89 lr: 3.148756458778214e-05\n",
      "Epoch 295: train loss 0.11438661302839007 val loss 0.6791033473773201  train acc 95.12  val acc 78.89 lr: 3.457042751172038e-05\n",
      "Epoch 296: train loss 0.10468673706054688 val loss 0.6820040029433145  train acc 96.79  val acc 78.61 lr: 3.7779245543570806e-05\n",
      "Epoch 297: train loss 0.11209150041852678 val loss 0.6852177429281643  train acc 96.55  val acc 78.61 lr: 4.109423525312721e-05\n",
      "Epoch 298: train loss 0.10733057657877604 val loss 0.6872297720892797  train acc 96.79  val acc 78.89 lr: 4.4494958626484106e-05\n",
      "Epoch 299: train loss 0.11179747808547247 val loss 0.6866722945238605  train acc 95.6  val acc 78.89 lr: 4.7960449073189464e-05\n",
      "Epoch 300: train loss 0.10782192775181361 val loss 0.6874160890435828  train acc 96.19  val acc 78.61 lr: 5.146934069224684e-05\n",
      "Epoch 301: train loss 0.10254596528552827 val loss 0.6886310910140918  train acc 96.9  val acc 78.61 lr: 5.499999999999971e-05\n",
      "Epoch 302: train loss 0.10379347120012555 val loss 0.6883357987806213  train acc 96.67  val acc 78.61 lr: 5.853065930775274e-05\n",
      "Epoch 303: train loss 0.10303038642519996 val loss 0.6843142427594813  train acc 97.14  val acc 78.33 lr: 6.20395509268101e-05\n",
      "Epoch 304: train loss 0.11291305905296689 val loss 0.6794809739529856  train acc 96.67  val acc 78.89 lr: 6.550504137351548e-05\n",
      "Epoch 305: train loss 0.09881974174862816 val loss 0.6723516482120202  train acc 96.67  val acc 79.44 lr: 6.890576474687239e-05\n",
      "Epoch 306: train loss 0.11434685843331473 val loss 0.6696762729059855  train acc 96.67  val acc 80.0 lr: 7.22207544564288e-05\n",
      "Epoch 307: train loss 0.11241154443650019 val loss 0.6691217001519024  train acc 96.67  val acc 80.0 lr: 7.542957248827938e-05\n",
      "Epoch 308: train loss 0.10275579180036272 val loss 0.6675872737492605  train acc 97.02  val acc 80.0 lr: 7.851243541221748e-05\n",
      "Epoch 309: train loss 0.10031654721214658 val loss 0.6670104682998378  train acc 96.79  val acc 80.0 lr: 8.145033635316106e-05\n",
      "Epoch 310: train loss 0.11867681230817523 val loss 0.6654733426376512  train acc 96.55  val acc 79.72 lr: 8.422516217485791e-05\n",
      "Epoch 311: train loss 0.10390709468296595 val loss 0.6647520485278956  train acc 96.43  val acc 79.72 lr: 8.68198051533943e-05\n",
      "Epoch 312: train loss 0.11080942608061291 val loss 0.6653694844296533  train acc 96.43  val acc 80.28 lr: 8.921826845200107e-05\n",
      "Epoch 313: train loss 0.12147820790608724 val loss 0.6683463677146363  train acc 95.95  val acc 80.28 lr: 9.140576474687232e-05\n",
      "Epoch 314: train loss 0.0986735843476795 val loss 0.6737828557212029  train acc 97.38  val acc 80.0 lr: 9.336880739593383e-05\n",
      "Epoch 315: train loss 0.10960212889171782 val loss 0.6827357530113216  train acc 96.43  val acc 78.89 lr: 9.509529358847617e-05\n",
      "Epoch 316: train loss 0.11687432243710473 val loss 0.687021856542079  train acc 96.07  val acc 79.17 lr: 9.65745789630076e-05\n",
      "Epoch 317: train loss 0.1156562986828032 val loss 0.6924181315489844  train acc 95.83  val acc 78.61 lr: 9.77975432332816e-05\n",
      "Epoch 318: train loss 0.1151584716070266 val loss 0.6979502624316852  train acc 95.95  val acc 78.61 lr: 9.875664641789512e-05\n",
      "Epoch 319: train loss 0.10553142910911924 val loss 0.7085720679834243  train acc 97.26  val acc 78.33 lr: 9.944597532678084e-05\n",
      "Epoch 320: train loss 0.10594469706217448 val loss 0.7232539785845596  train acc 97.14  val acc 77.78 lr: 9.98612800179904e-05\n",
      "Epoch 321: train loss 0.10522078559512184 val loss 0.7230008260058605  train acc 96.67  val acc 78.33 lr: 9.999999999999964e-05\n",
      "Epoch 322: train loss 0.10755114782424201 val loss 0.715317280331273  train acc 96.19  val acc 78.06 lr: 9.98612800179904e-05\n",
      "Epoch 323: train loss 0.0993046170189267 val loss 0.7090252917007777  train acc 96.43  val acc 78.61 lr: 9.944597532678084e-05\n",
      "Epoch 324: train loss 0.11322008768717448 val loss 0.7050318892666678  train acc 95.95  val acc 78.61 lr: 9.875664641789512e-05\n",
      "Epoch 325: train loss 0.11586858658563523 val loss 0.702896615481163  train acc 95.71  val acc 78.61 lr: 9.779754323328161e-05\n",
      "Epoch 326: train loss 0.08823912484305245 val loss 0.7076544382046556  train acc 97.98  val acc 78.33 lr: 9.657457896300761e-05\n",
      "Epoch 327: train loss 0.10389600481305804 val loss 0.7123590028704738  train acc 96.9  val acc 77.78 lr: 9.509529358847627e-05\n",
      "Epoch 328: train loss 0.10275434766496931 val loss 0.7106713785235148  train acc 96.19  val acc 78.06 lr: 9.336880739593387e-05\n",
      "Epoch 329: train loss 0.09530140104747954 val loss 0.7134024638901741  train acc 97.74  val acc 77.78 lr: 9.140576474687246e-05\n",
      "Epoch 330: train loss 0.10660841805594308 val loss 0.720003098333727  train acc 96.31  val acc 78.06 lr: 8.921826845200112e-05\n",
      "Epoch 331: train loss 0.11572693416050502 val loss 0.7336954284799423  train acc 96.07  val acc 78.06 lr: 8.681980515339447e-05\n",
      "Epoch 332: train loss 0.10512533641996838 val loss 0.7439066160102529  train acc 96.67  val acc 77.5 lr: 8.422516217485798e-05\n",
      "Epoch 333: train loss 0.10171883900960287 val loss 0.7471307437949073  train acc 96.31  val acc 76.94 lr: 8.1450336353161e-05\n",
      "Epoch 334: train loss 0.11102273123604911 val loss 0.7423975817634411  train acc 96.19  val acc 77.22 lr: 7.851243541221755e-05\n",
      "Epoch 335: train loss 0.11218379792712983 val loss 0.7329422398570256  train acc 96.07  val acc 77.78 lr: 7.542957248827932e-05\n",
      "Epoch 336: train loss 0.10580379849388485 val loss 0.7218885857896096  train acc 95.95  val acc 77.78 lr: 7.222075445642889e-05\n",
      "Epoch 337: train loss 0.09493098486037481 val loss 0.7169759597062557  train acc 96.79  val acc 78.06 lr: 6.890576474687247e-05\n",
      "Epoch 338: train loss 0.10343604314894904 val loss 0.7072667676017216  train acc 96.79  val acc 79.17 lr: 6.550504137351574e-05\n",
      "Epoch 339: train loss 0.10804147266206288 val loss 0.702504376391962  train acc 96.31  val acc 79.17 lr: 6.203955092681021e-05\n",
      "Epoch 340: train loss 0.11276444026402065 val loss 0.6965430390236109  train acc 95.83  val acc 79.17 lr: 5.8530659307753e-05\n",
      "Epoch 341: train loss 0.10542356400262742 val loss 0.6945449114507118  train acc 96.31  val acc 79.17 lr: 5.499999999999981e-05\n",
      "Epoch 342: train loss 0.09680272965204148 val loss 0.6904408042470845  train acc 96.79  val acc 79.72 lr: 5.146934069224694e-05\n",
      "Epoch 343: train loss 0.10204508645193917 val loss 0.6897621914387179  train acc 96.19  val acc 79.44 lr: 4.7960449073189566e-05\n",
      "Epoch 344: train loss 0.09358573186965216 val loss 0.6910289516584827  train acc 96.31  val acc 79.17 lr: 4.4494958626484045e-05\n",
      "Epoch 345: train loss 0.11762704395112537 val loss 0.6917167907394861  train acc 95.95  val acc 79.17 lr: 4.1094235253127305e-05\n",
      "Epoch 346: train loss 0.10360037485758464 val loss 0.6949743753182009  train acc 96.43  val acc 78.89 lr: 3.7779245543570894e-05\n",
      "Epoch 347: train loss 0.10193192618233816 val loss 0.6948660621861339  train acc 97.14  val acc 78.89 lr: 3.4570427511720465e-05\n",
      "Epoch 348: train loss 0.10859675634474981 val loss 0.6989638052800061  train acc 96.31  val acc 78.89 lr: 3.148756458778222e-05\n",
      "Epoch 349: train loss 0.09632942563011533 val loss 0.701467307445625  train acc 96.67  val acc 78.89 lr: 2.854966364683875e-05\n",
      "Epoch 350: train loss 0.09809138888404483 val loss 0.7015937870217676  train acc 97.02  val acc 79.17 lr: 2.5774837825141642e-05\n",
      "Epoch 351: train loss 0.09139017377580916 val loss 0.7037387429743951  train acc 97.38  val acc 79.17 lr: 2.318019484660538e-05\n",
      "Epoch 352: train loss 0.09454060509091332 val loss 0.7028790084166897  train acc 96.9  val acc 79.17 lr: 2.078173154799862e-05\n",
      "Epoch 353: train loss 0.10800705864315942 val loss 0.702160549299388  train acc 96.19  val acc 79.17 lr: 1.8594235253127463e-05\n",
      "Epoch 354: train loss 0.10086313883463542 val loss 0.7026287907831027  train acc 96.07  val acc 79.17 lr: 1.6631192604065845e-05\n",
      "Epoch 355: train loss 0.11196036565871466 val loss 0.7016852955519686  train acc 96.07  val acc 79.17 lr: 1.4904706411523434e-05\n",
      "Epoch 356: train loss 0.11139089493524461 val loss 0.7017931574652322  train acc 96.43  val acc 79.17 lr: 1.342542103699214e-05\n",
      "Epoch 357: train loss 0.10033013480050224 val loss 0.7026062463642511  train acc 96.67  val acc 79.17 lr: 1.2202456766718074e-05\n",
      "Epoch 358: train loss 0.11901384989420573 val loss 0.6997936713175792  train acc 95.83  val acc 79.17 lr: 1.1243353582104577e-05\n",
      "Epoch 359: train loss 0.09173582167852493 val loss 0.6989811907121682  train acc 97.5  val acc 79.17 lr: 1.055402467321879e-05\n",
      "Epoch 360: train loss 0.11521422976539249 val loss 0.6992642628168085  train acc 95.83  val acc 79.17 lr: 1.0138719982009247e-05\n",
      "Epoch 361: train loss 0.09104984828404018 val loss 0.6973263963942685  train acc 97.02  val acc 79.17 lr: 1e-05\n",
      "Epoch 362: train loss 0.1037691388811384 val loss 0.6948890172298811  train acc 96.79  val acc 79.17 lr: 1.0138719982009242e-05\n",
      "Epoch 363: train loss 0.10662450336274647 val loss 0.6935388977801319  train acc 96.55  val acc 79.17 lr: 1.0554024673218859e-05\n",
      "Epoch 364: train loss 0.09835110618954614 val loss 0.6923278899128962  train acc 96.43  val acc 79.17 lr: 1.1243353582104685e-05\n",
      "Epoch 365: train loss 0.10914646330333891 val loss 0.6914173174843832  train acc 95.95  val acc 79.44 lr: 1.2202456766718281e-05\n",
      "Epoch 366: train loss 0.09395890917096819 val loss 0.6912686714740087  train acc 96.79  val acc 79.44 lr: 1.3425421036992474e-05\n",
      "Epoch 367: train loss 0.09879427410307384 val loss 0.6886998555848292  train acc 97.02  val acc 80.0 lr: 1.4904706411523916e-05\n",
      "Epoch 368: train loss 0.09973624093191964 val loss 0.6868087323417902  train acc 97.26  val acc 80.28 lr: 1.663119260406659e-05\n",
      "Epoch 369: train loss 0.09381560371035622 val loss 0.6864528497510561  train acc 96.9  val acc 80.28 lr: 1.8594235253128235e-05\n",
      "Epoch 370: train loss 0.08370186941964286 val loss 0.6878694621295508  train acc 97.62  val acc 80.83 lr: 2.0781731547999718e-05\n",
      "Epoch 371: train loss 0.11954823448544456 val loss 0.6891529390626461  train acc 95.95  val acc 79.72 lr: 2.318019484660662e-05\n",
      "Epoch 372: train loss 0.09021700904482886 val loss 0.688588460358628  train acc 97.38  val acc 80.0 lr: 2.577483782514339e-05\n",
      "Epoch 373: train loss 0.10011630285353888 val loss 0.689279781578168  train acc 96.55  val acc 80.0 lr: 2.8549663646840546e-05\n",
      "Epoch 374: train loss 0.11082247779482887 val loss 0.6893096966269412  train acc 96.07  val acc 79.72 lr: 3.1487564587784454e-05\n",
      "Epoch 375: train loss 0.10361105600992838 val loss 0.692290058798858  train acc 96.9  val acc 79.17 lr: 3.457042751172303e-05\n",
      "Epoch 376: train loss 0.09946915762765067 val loss 0.6931984023499416  train acc 96.31  val acc 79.17 lr: 3.7779245543573814e-05\n",
      "Epoch 377: train loss 0.09382579440162295 val loss 0.6942088000948498  train acc 97.02  val acc 79.72 lr: 4.1094235253130734e-05\n",
      "Epoch 378: train loss 0.09442914326985677 val loss 0.6955480736660783  train acc 96.67  val acc 79.44 lr: 4.449495862648785e-05\n",
      "Epoch 379: train loss 0.10528902326311385 val loss 0.6973328363027709  train acc 96.55  val acc 78.89 lr: 4.7960449073193584e-05\n",
      "Epoch 380: train loss 0.09645572844005766 val loss 0.6994046983774662  train acc 97.5  val acc 78.61 lr: 5.1469340692251176e-05\n",
      "Epoch 381: train loss 0.11231603168305897 val loss 0.7045467335413743  train acc 95.95  val acc 78.61 lr: 5.500000000000475e-05\n",
      "Epoch 382: train loss 0.08689847673688617 val loss 0.7023973363883776  train acc 97.86  val acc 78.89 lr: 5.8530659307758e-05\n",
      "Epoch 383: train loss 0.09945868537539528 val loss 0.6997841044071385  train acc 96.19  val acc 78.89 lr: 6.203955092681577e-05\n",
      "Epoch 384: train loss 0.10462644667852493 val loss 0.6977730485715031  train acc 95.48  val acc 78.89 lr: 6.550504137352135e-05\n",
      "Epoch 385: train loss 0.11256796518961588 val loss 0.698164779902006  train acc 96.19  val acc 78.61 lr: 6.890576474687878e-05\n",
      "Epoch 386: train loss 0.09650319417317708 val loss 0.6990965610627531  train acc 96.9  val acc 78.61 lr: 7.222075445643569e-05\n",
      "Epoch 387: train loss 0.10280513763427734 val loss 0.7022448653787973  train acc 96.43  val acc 78.61 lr: 7.542957248828647e-05\n",
      "Epoch 388: train loss 0.10710996900285993 val loss 0.7089135326684909  train acc 96.07  val acc 78.89 lr: 7.851243541222491e-05\n",
      "Epoch 389: train loss 0.10263465699695405 val loss 0.7162222691829562  train acc 96.67  val acc 78.06 lr: 8.14503363531687e-05\n",
      "Epoch 390: train loss 0.10487434750511533 val loss 0.7290125560658374  train acc 97.14  val acc 78.33 lr: 8.42251621748661e-05\n",
      "Epoch 391: train loss 0.08497894832066127 val loss 0.743159552367451  train acc 97.5  val acc 77.78 lr: 8.681980515340265e-05\n",
      "Epoch 392: train loss 0.09116139184860957 val loss 0.7486054196820568  train acc 96.9  val acc 77.5 lr: 8.921826845200966e-05\n",
      "Epoch 393: train loss 0.0991938000633603 val loss 0.7620240462618775  train acc 96.19  val acc 76.67 lr: 9.140576474688107e-05\n",
      "Epoch 394: train loss 0.10091419219970703 val loss 0.7615977922078863  train acc 96.9  val acc 76.67 lr: 9.336880739594287e-05\n",
      "Epoch 395: train loss 0.09831737336658296 val loss 0.7545776201588825  train acc 97.38  val acc 77.5 lr: 9.50952935884854e-05\n",
      "Epoch 396: train loss 0.11107947939918154 val loss 0.7342661199075272  train acc 96.07  val acc 78.33 lr: 9.6574578963017e-05\n",
      "Epoch 397: train loss 0.09144731249128069 val loss 0.7125808164040373  train acc 97.26  val acc 78.61 lr: 9.779754323329113e-05\n",
      "Epoch 398: train loss 0.0902373540969122 val loss 0.6970147880309157  train acc 97.5  val acc 79.17 lr: 9.875664641790472e-05\n",
      "Epoch 399: train loss 0.09168620336623419 val loss 0.6832433715075591  train acc 97.26  val acc 80.56 lr: 9.94459753267906e-05\n",
      "Epoch 400: train loss 0.09625920795259021 val loss 0.6786210105640934  train acc 96.55  val acc 80.28 lr: 9.986128001800019e-05\n",
      "Epoch 401: train loss 0.0999038060506185 val loss 0.6758553891490793  train acc 97.02  val acc 81.11 lr: 0.00010000000000000944\n",
      "Epoch 402: train loss 0.08642800649007161 val loss 0.6745012115702937  train acc 98.1  val acc 81.67 lr: 9.98612800180002e-05\n",
      "Epoch 403: train loss 0.09225801740373885 val loss 0.6759738604683495  train acc 97.5  val acc 81.94 lr: 9.944597532679058e-05\n",
      "Epoch 404: train loss 0.08852999550955636 val loss 0.6783706644419782  train acc 97.14  val acc 81.39 lr: 9.87566464179048e-05\n",
      "Epoch 405: train loss 0.09948297228131975 val loss 0.6839832917439482  train acc 96.79  val acc 80.83 lr: 9.779754323329111e-05\n",
      "Epoch 406: train loss 0.09227948869977678 val loss 0.6925567944671148  train acc 97.5  val acc 80.28 lr: 9.657457896301703e-05\n",
      "Epoch 407: train loss 0.09041898818243117 val loss 0.7086612162187077  train acc 97.26  val acc 79.44 lr: 9.509529358848551e-05\n",
      "Epoch 408: train loss 0.09080703372047061 val loss 0.7368805990322822  train acc 96.55  val acc 78.89 lr: 9.336880739594285e-05\n",
      "Epoch 409: train loss 0.08666151137579055 val loss 0.7718011387635392  train acc 97.02  val acc 77.78 lr: 9.140576474688128e-05\n",
      "Epoch 410: train loss 0.08872853233700707 val loss 0.8054642168072159  train acc 97.26  val acc 76.67 lr: 8.921826845200973e-05\n",
      "Epoch 411: train loss 0.08246964045933315 val loss 0.843684958590168  train acc 97.74  val acc 76.39 lr: 8.681980515340282e-05\n",
      "Epoch 412: train loss 0.08650939578101749 val loss 0.8669825355826254  train acc 97.26  val acc 75.83 lr: 8.422516217486604e-05\n",
      "Epoch 413: train loss 0.07801021394275484 val loss 0.8775639976283713  train acc 97.5  val acc 75.56 lr: 8.14503363531689e-05\n",
      "Epoch 414: train loss 0.08432538168770927 val loss 0.8700572291403947  train acc 97.14  val acc 75.83 lr: 7.851243541222484e-05\n",
      "Epoch 415: train loss 0.09989391508556547 val loss 0.8520176804581827  train acc 97.14  val acc 76.11 lr: 7.542957248828655e-05\n",
      "Epoch 416: train loss 0.08209336598714193 val loss 0.8296439627670423  train acc 97.38  val acc 76.67 lr: 7.222075445643549e-05\n",
      "Epoch 417: train loss 0.09238749004545665 val loss 0.8046230845924454  train acc 96.79  val acc 76.94 lr: 6.890576474687887e-05\n",
      "Epoch 418: train loss 0.08984465826125372 val loss 0.7842850901584364  train acc 97.26  val acc 76.67 lr: 6.550504137352177e-05\n",
      "Epoch 419: train loss 0.08361649286179315 val loss 0.7644470108895857  train acc 97.74  val acc 78.06 lr: 6.203955092681588e-05\n",
      "Epoch 420: train loss 0.09332882109142485 val loss 0.7491969134933097  train acc 97.02  val acc 78.61 lr: 5.853065930775829e-05\n",
      "Epoch 421: train loss 0.08760365077427455 val loss 0.7329747720341832  train acc 97.02  val acc 79.17 lr: 5.500000000000472e-05\n",
      "Epoch 422: train loss 0.09321424393426804 val loss 0.7201155294371507  train acc 97.74  val acc 79.17 lr: 5.146934069225146e-05\n",
      "Epoch 423: train loss 0.08675326392764136 val loss 0.7099710839175432  train acc 96.55  val acc 79.17 lr: 4.796044907319355e-05\n",
      "Epoch 424: train loss 0.09188225155784971 val loss 0.7046124040805846  train acc 97.26  val acc 79.72 lr: 4.449495862648797e-05\n",
      "Epoch 425: train loss 0.08743039994012741 val loss 0.6982095431531328  train acc 97.14  val acc 80.56 lr: 4.109423525313055e-05\n",
      "Epoch 426: train loss 0.09966288067045666 val loss 0.6977838919082789  train acc 96.19  val acc 80.28 lr: 3.777924554357393e-05\n",
      "Epoch 427: train loss 0.10022595723470053 val loss 0.6969586651085592  train acc 96.19  val acc 80.56 lr: 3.457042751172315e-05\n",
      "Epoch 428: train loss 0.0853902362641834 val loss 0.697630766626825  train acc 97.26  val acc 80.56 lr: 3.148756458778457e-05\n",
      "Epoch 429: train loss 0.0929013660975865 val loss 0.6985291463414944  train acc 96.9  val acc 80.28 lr: 2.8549663646840784e-05\n",
      "Epoch 430: train loss 0.08931238083612351 val loss 0.6985492430221414  train acc 97.02  val acc 80.0 lr: 2.577483782514337e-05\n",
      "Epoch 431: train loss 0.08401679084414528 val loss 0.7011111418945043  train acc 97.26  val acc 80.0 lr: 2.318019484660682e-05\n",
      "Epoch 432: train loss 0.10251717340378534 val loss 0.7020805027833045  train acc 96.67  val acc 79.72 lr: 2.078173154799969e-05\n",
      "Epoch 433: train loss 0.08719792138962519 val loss 0.7058235957737173  train acc 97.38  val acc 79.17 lr: 1.8594235253128307e-05\n",
      "Epoch 434: train loss 0.08799700055803572 val loss 0.7048081914149851  train acc 97.14  val acc 78.61 lr: 1.663119260406649e-05\n",
      "Epoch 435: train loss 0.0885773431687128 val loss 0.7057264890528566  train acc 97.14  val acc 78.61 lr: 1.4904706411523974e-05\n",
      "Epoch 436: train loss 0.09501593453543526 val loss 0.7066555628362947  train acc 96.43  val acc 78.61 lr: 1.342542103699252e-05\n",
      "Epoch 437: train loss 0.08591071537562779 val loss 0.7083098610727075  train acc 96.67  val acc 78.61 lr: 1.2202456766718415e-05\n",
      "Epoch 438: train loss 0.08872716994512649 val loss 0.7100638089928315  train acc 97.5  val acc 78.61 lr: 1.1243353582104712e-05\n",
      "Epoch 439: train loss 0.08828686305454798 val loss 0.7129302604512374  train acc 97.74  val acc 78.61 lr: 1.0554024673218854e-05\n",
      "Epoch 440: train loss 0.08562887282598586 val loss 0.7129317433974447  train acc 98.21  val acc 78.61 lr: 1.0138719982009262e-05\n",
      "Epoch 441: train loss 0.0884467533656529 val loss 0.7118490762041935  train acc 97.26  val acc 78.61 lr: 1e-05\n",
      "Epoch 442: train loss 0.08638806116013299 val loss 0.7094525859775094  train acc 97.38  val acc 78.61 lr: 1.0138719982009242e-05\n",
      "Epoch 443: train loss 0.0817342758178711 val loss 0.7112099272092018  train acc 97.38  val acc 78.61 lr: 1.0554024673218844e-05\n",
      "Epoch 444: train loss 0.08046238308861142 val loss 0.7116336413581621  train acc 97.74  val acc 78.61 lr: 1.1243353582104595e-05\n",
      "Epoch 445: train loss 0.0901397977556501 val loss 0.7109843301907384  train acc 96.9  val acc 78.61 lr: 1.2202456766718117e-05\n",
      "Epoch 446: train loss 0.08366050720214843 val loss 0.7147183674470419  train acc 97.5  val acc 78.61 lr: 1.3425421036992097e-05\n",
      "Epoch 447: train loss 0.08819225856236049 val loss 0.7158563930994785  train acc 96.79  val acc 78.61 lr: 1.4904706411523557e-05\n",
      "Epoch 448: train loss 0.09237797146751767 val loss 0.7145701276389169  train acc 96.79  val acc 78.89 lr: 1.6631192604065936e-05\n",
      "Epoch 449: train loss 0.0907927740187872 val loss 0.7135820871133851  train acc 97.02  val acc 78.89 lr: 1.859423525312761e-05\n",
      "Epoch 450: train loss 0.08816184997558593 val loss 0.7129807312704451  train acc 96.9  val acc 78.89 lr: 2.0781731547999034e-05\n"
     ]
    }
   ],
   "source": [
    "# 训练参数\n",
    "n_epochs = 450\n",
    "learning_rate = 1e-4\n",
    "min_learning_rate = 1e-8\n",
    "batch_size = 1024\n",
    "decay_steps = 20\n",
    "lamda = 0.05\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "model, history = train_model(\n",
    "  model, \n",
    "  train_loader, \n",
    "  val_loader, \n",
    "  n_epochs=n_epochs,\n",
    "  learning_rate=learning_rate,\n",
    "  min_lr=min_learning_rate,\n",
    "  lamda=lamda,\n",
    "  decay_steps=decay_steps,\n",
    "  is_ae=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbd5b8a6-6ba6-407a-9fb2-135d327527a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc is 0.8305555555555556\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"CNN_LSTM_128_64_60_20_acc83.pt\"))\n",
    "\n",
    "acc, labels, losses = cal_acc_batch(model, val_dir + \"/data.npy\")\n",
    "\n",
    "print(f\"Acc is {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e68921a-fb7d-4481-a754-49dcf826799a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f1fa4f8b3d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.displot(losses, bins=50, kde=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "895f5353-0e5b-4e16-a55c-0a7c8aa63fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAokAAAJSCAYAAACvLl/NAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABc+ElEQVR4nO3deVhU5f//8dcMyoAgIKgs5q4h5q6laKUm5YZpWmZZopm2qC22Wu6WlGVumUu5p2WLWllZJqb1CfdQMzXNpUVxQUFBQJbz+8Of8208uKAMA87z0TXXFfc5c+Y9ZzLfvO77nLEYhmEIAAAA+A+rqwsAAABA0UOTCAAAABOaRAAAAJjQJAIAAMCEJhEAAAAmNIkAAAAwoUkEAACACU0iAAAATGgSAQAAYEKTCAAAABOaRLit+Ph4eXh4qGPHjq4upUj566+/1LFjR5UqVUrly5fXCy+8oOzs7Es+548//lDnzp1VtmxZ+fn56dZbb9Xq1avt25OSktSuXTuFhYXJZrOpYsWKGjhwoE6dOuXstwMAuEo0iXBbs2bN0qBBg7R27VodOnTIpbWcPXvWpa9/Xk5Ojjp27KizZ8/ql19+0bx58zR37lwNHz78ks+Ljo5Wdna24uLitHnzZtWvX1/R0dFKTEyUJFmtVnXu3Flffvml/vjjD82dO1c//PCDHn/88cJ4WwCAq2EAbuj06dOGr6+vsWvXLuP+++83Xn/9ddM+X375pdGkSRPDZrMZQUFBRpcuXezbMjIyjBdffNG44YYbDE9PT6N69erGBx98YBiGYcyZM8fw9/d3ONbSpUuN//5xGzFihFG/fn3j/fffN6pUqWJYLBbDMAzj22+/NVq0aGH4+/sbgYGBRseOHY29e/c6HOvvv/82evToYZQpU8YoVaqU0bhxY2PdunXG/v37DYvFYmzcuNFh/wkTJhiVKlUycnJyLntevvnmG8NqtRqJiYn2sWnTphl+fn5GZmZmns85duyYIclYu3atfezUqVOGJGPlypUXfa1JkyYZN9xww2VrAgC4Bkki3NInn3yiWrVqKTw8XA899JBmz54twzDs27/++mvdc8896tChg3799VetWrVKt9xyi317r1699NFHH2ny5MnauXOnZsyYIV9f33zVsHfvXn3++edasmSJEhISJElpaWkaPHiwNm3apFWrVslqteqee+5Rbm6uJCk1NVUtW7bUv//+qy+//FJbt27Viy++qNzcXFWpUkVRUVGaM2eOw+vMmTNHvXv3ltVqVZUqVTRy5MiL1hQfH6+6desqODjYPta2bVudOnVKO3bsyPM5QUFBCg8P1/z585WWlqbs7GzNmDFD5cuXV+PGjfN8zqFDh7RkyRK1bNkyH2cMAFCoXN2lAq7QvHlzY+LEiYZhGEZWVpZRtmxZY/Xq1fbtkZGRRs+ePfN87u7duy+Zkl1pkliyZEnj6NGjl6zzfEq3fft2wzAMY8aMGUbp0qWNpKSkPPdfvHixUaZMGSMjI8MwDMPYvHmzYbFYjP379xuGYRh33HGHMWXKlIu+Xr9+/Yy77rrLYSwtLc2QZHzzzTcXfd7ff/9tNG7c2LBYLIaHh4cRGhpqbNmyxbRfjx49DG9vb0OS0alTJyM9Pf2S7x8A4DokiXA7u3fv1oYNG/TAAw9IkkqUKKH7779fs2bNsu+TkJCgNm3a5Pn8hIQEeXh4XHMKVrlyZZUrV85hbM+ePXrggQdUrVo1+fn5qUqVKpLOXUxy/rUbNmyowMDAPI/ZpUsXeXh4aOnSpZKkuXPnqnXr1vbjrFq1SgMHDrymui9kGIYGDBig8uXL66efftKGDRvUpUsXderUSYcPH3bYd8KECdqyZYu++OIL/fnnnxo8eHCB1gIAKDglXF0AUNhmzZql7OxshYWF2ccMw5DNZtO7774rf39/eXt7X/T5l9omnbtIw/jP1LUkZWVlmfbz8fExjXXq1EmVK1fW+++/r7CwMOXm5qpOnTr2C1su99qenp7q1auX5syZo65du2rRokWaNGnSJZ/zXyEhIdqwYYPD2JEjR+zb8hIXF6fly5fr5MmT8vPzkyS99957WrlypebNm6eXX37Z4fghISGqVauWAgMDddttt2nYsGEKDQ294hoBAIWDJBFuJTs7W/Pnz9f48eOVkJBgf2zdulVhYWH66KOPJEn16tXTqlWr8jxG3bp1lZubqzVr1uS5vVy5cjp9+rTS0tLsY+fXHF5KUlKSdu/eraFDh6pNmzaKiIjQyZMnHfapV6+eEhISdOLEiYse59FHH9UPP/yg9957T9nZ2eratetlX/u8yMhIbd++XUePHrWPrVy5Un5+fqpdu3aezzlz5oykc83xf1mtVvtayryc35aZmXnF9QEACpGLp7uBQrV06VLD09PTSE5ONm178cUXjSZNmhiGYRirV682rFarMXz4cOP33383tm3bZrzxxhv2fXv37m1UrFjRWLp0qbFv3z5j9erVxuLFiw3DMIykpCTDx8fHeOqpp4y9e/caCxcuNMLCwvK8uvm/cnJyjKCgIOOhhx4y9uzZY6xatcq4+eabDUnG0qVLDcMwjMzMTOPGG280brvtNuPnn382/vzzT+Ozzz4zfvnlF4djNW/e3PD09DQef/xxh/HLrUnMzs426tSpY9x1111GQkKCsWLFCqNcuXLGkCFD7PusX7/eCA8PN/755x/DMM6tmwwKCjK6du1qJCQkGLt37zaef/55o2TJkkZCQoJhGIbx9ddfG7Nnzza2b99u7N+/31i+fLkRERFhtGjR4qK1AABciyYRbiU6Otro0KFDntvWr19vSDK2bt1qGIZhfP7550aDBg0MT09Po2zZskbXrl3t+6anpxvPPvusERoaanh6eho1atQwZs+ebd++dOlSo0aNGoa3t7cRHR1tzJw587JNomEYxsqVK42IiAjDZrMZ9erVM3788UeHJtEwDOPAgQNGt27dDD8/P6NUqVJGkyZNjPXr1zscZ9asWYYkY8OGDQ7jlStXNkaMGHHJc3TgwAGjffv2hre3t1G2bFnjueeeM7KysuzbV69ebUiyXwxjGIaxceNG46677jICAwON0qVLG82aNXO40CUuLs6IjIw0/P39DS8vL6NmzZrGSy+9ZJw8efKStQAAXMdiGBcsngJQ7I0ZM0affvqptm3b5upSAADFFGsSgetIamqqfvvtN7377rsaNGiQq8sBABRjNInAdWTgwIFq3LixWrVqpUceecTV5QAAijGmmwEAAGBCkggAAAATmkQAAACY0CQCAADAhCYRAAAAJjSJAAAAMKFJBAAAgAlNIgAAAExoEgEAAGBCkwgAAAATmkQAAACY0CQCAADAhCYRAAAAJjSJAAAAMKFJBAAAgAlNIgAAAExoEgEAAGBCkwgAAAATmkQAAACY0CQCAADAhCYRAAAAJjSJAAAAMKFJBAAAgAlNIgAAAExoEgEAAGBCkwgAAAATmkQAAACY0CQCAADAhCYRAAAAJjSJAAAAMKFJBAAAgAlNIgAAAExoEgEX2rNnj+666y75+/vLYrFo2bJlBXr8AwcOyGKxaO7cuQV63OKsVatWatWqlavLAIAijyYRbu/PP//UY489pmrVqsnLy0t+fn5q0aKFJk2apPT0dKe+dkxMjLZv367XX39dCxYsUJMmTZz6eoWpd+/eslgs8vPzy/M87tmzRxaLRRaLRW+//Xa+j3/o0CGNHDlSCQkJBVDt1bNYLBo4cGCe2+bOnSuLxaJNmzY57fWLynkAcP0p4eoCAFf6+uuvdd9998lms6lXr16qU6eOzp49q59//lkvvPCCduzYoZkzZzrltdPT0xUfH69XX331ok3GtapcubLS09NVsmRJpxz/ckqUKKEzZ87oq6++Uvfu3R22LVy4UF5eXsrIyLiqYx86dEijRo1SlSpV1KBBgyt+3vfff39Vr1dUXe15AIDLoUmE29q/f7969OihypUrKy4uTqGhofZtAwYM0N69e/X111877fWPHTsmSQoICHDaa1gsFnl5eTnt+Jdjs9nUokULffTRR6YmcdGiRerYsaM+//zzQqnlzJkzKlWqlDw9PQvl9QCguGO6GW5r3LhxSk1N1axZsxwaxPNq1Kihp59+2v5zdna2xowZo+rVq8tms6lKlSp65ZVXlJmZ6fC8KlWqKDo6Wj///LNuueUWeXl5qVq1apo/f759n5EjR6py5cqSpBdeeEEWi0VVqlSRdG6a9vy//9fIkSNlsVgcxlauXKlbb71VAQEB8vX1VXh4uF555RX79outSYyLi9Ntt90mHx8fBQQEqHPnztq5c2eer7d371717t1bAQEB8vf3V58+fXTmzJmLn9gLPPjgg/r222+VnJxsH9u4caP27NmjBx980LT/iRMn9Pzzz6tu3bry9fWVn5+f2rdvr61bt9r3+fHHH3XzzTdLkvr06WOftj7/Plu1aqU6depo8+bNuv3221WqVCn7eblwTWJMTIy8vLxM779t27YqU6aMDh06dMXv9Urt2rVL9957rwIDA+Xl5aUmTZroyy+/dNp52LZtm1q2bKlSpUqpRo0a+uyzzyRJa9asUdOmTeXt7a3w8HD98MMPDjUcPHhQTz75pMLDw+Xt7a2goCDdd999OnDggMN+56fV165dq8cee0xBQUHy8/NTr169dPLkyQI+ewAKC00i3NZXX32latWqqXnz5le0/6OPPqrhw4erUaNGmjBhglq2bKnY2Fj16NHDtO/evXt177336s4779T48eNVpkwZ9e7dWzt27JAkde3aVRMmTJAkPfDAA1qwYIEmTpyYr/p37Nih6OhoZWZmavTo0Ro/frzuvvtu/e9//7vk83744Qe1bdtWR48e1ciRIzV48GD98ssvatGihekvf0nq3r27Tp8+rdjYWHXv3l1z587VqFGjrrjOrl27ymKxaMmSJfaxRYsWqVatWmrUqJFp/3379mnZsmWKjo7WO++8oxdeeEHbt29Xy5Yt7Q1bRESERo8eLUnq37+/FixYoAULFuj222+3HycpKUnt27dXgwYNNHHiRLVu3TrP+iZNmqRy5copJiZGOTk5kqQZM2bo+++/15QpUxQWFnbZ95iRkaHjx4+bHqmpqaZ9d+zYoWbNmmnnzp16+eWXNX78ePn4+KhLly5aunRpgZ+HkydPKjo6Wk2bNtW4ceNks9nUo0cPLV68WD169FCHDh30xhtvKC0tTffee69Onz5tf+7GjRv1yy+/qEePHpo8ebIef/xxrVq1Sq1atcrzF4WBAwdq586dGjlypHr16qWFCxeqS5cuMgzjsucQQBFkAG4oJSXFkGR07tz5ivZPSEgwJBmPPvqow/jzzz9vSDLi4uLsY5UrVzYkGWvXrrWPHT161LDZbMZzzz1nH9u/f78hyXjrrbccjhkTE2NUrlzZVMOIESOM//6RnTBhgiHJOHbs2EXrPv8ac+bMsY81aNDAKF++vJGUlGQf27p1q2G1Wo1evXqZXu+RRx5xOOY999xjBAUFXfQ1//s+fHx8DMMwjHvvvddo06aNYRiGkZOTY4SEhBijRo3K8xxkZGQYOTk5pvdhs9mM0aNH28c2btxoem/ntWzZ0pBkTJ8+Pc9tLVu2dBj77rvvDEnGa6+9Zuzbt8/w9fU1unTpctn3aBiGIemyj40bN9r3b9OmjVG3bl0jIyPDPpabm2s0b97cqFmzplPOw6JFi+xju3btMiQZVqvVWLdunekc/Pc4Z86cMR0zPj7ekGTMnz/fPjZnzhxDktG4cWPj7Nmz9vFx48YZkowvvvjiYqcPQBFGkgi3dOrUKUlS6dKlr2j/b775RpI0ePBgh/HnnntOkkxrF2vXrq3bbrvN/nO5cuUUHh6uffv2XXXNFzq/lvGLL75Qbm7uFT3n8OHDSkhIUO/evRUYGGgfr1evnu688077+/yvxx9/3OHn2267TUlJSfZzeCUefPBB/fjjj0pMTFRcXJwSExPznGqWzq1jtFrP/a8pJydHSUlJ9qn0LVu2XPFr2mw29enT54r2veuuu/TYY49p9OjR6tq1q7y8vDRjxowrfq3OnTtr5cqVpscLL7zgsN+JEycUFxdnT2fPJ45JSUlq27at9uzZo3///ddef0GcB19fX4e0Ozw8XAEBAYqIiFDTpk3t4+f//b//jXp7e9v/PSsrS0lJSapRo4YCAgLyrKF///4OF0k98cQTKlGiRJ7/XQEo+mgS4Zb8/PwkyWFq7VIOHjwoq9WqGjVqOIyHhIQoICBABw8edBivVKmS6RhlypQp0PVZ999/v1q0aKFHH31UwcHB6tGjhz755JNLNozn6wwPDzdti4iI0PHjx5WWluYwfuF7KVOmjCTl67106NBBpUuX1uLFi7Vw4ULdfPPNpnN5Xm5uriZMmKCaNWvKZrOpbNmyKleunLZt26aUlJQrfs0KFSrk6yKVt99+W4GBgUpISNDkyZNVvnz5K37uDTfcoKioKNOjdu3aDvvt3btXhmFo2LBhKleunMNjxIgRkqSjR49KKrjzcMMNN5jWsvr7+6tixYqmMcnxc01PT9fw4cNVsWJFhxqSk5PzrKFmzZoOP/v6+io0NDTPZQwAij6uboZb8vPzU1hYmH777bd8Pe/Cv2wvxsPDI89x4wrWZl3sNc6vlzvP29tba9eu1erVq/X1119rxYoVWrx4se644w59//33F60hv67lvZxns9nUtWtXzZs3T/v27dPIkSMvuu/YsWM1bNgwPfLIIxozZowCAwNltVr1zDPPXHFiKjmmYFfi119/tTdo27dv1wMPPJCv51+J8/U///zzatu2bZ77nG+eC+o8XOzzu5LPddCgQZozZ46eeeYZRUZG2m/63qNHj3zVAKB4okmE24qOjtbMmTMVHx+vyMjIS+5buXJl5ebmas+ePYqIiLCPHzlyRMnJyfYrlQtCmTJlHK4EPu/CtFKSrFar2rRpozZt2uidd97R2LFj9eqrr2r16tWKiorK831I0u7du03bdu3apbJly8rHx+fa30QeHnzwQc2ePVtWqzXPi33O++yzz9S6dWvNmjXLYTw5OVlly5a1/3ylDfuVSEtLU58+fVS7dm01b95c48aN0z333GO/crigVKtWTZJUsmTJPD+f/3LFecirhpiYGI0fP94+lpGRked/n9K5G6T/9wKh1NRUHT58WB06dHBajQCch+lmuK0XX3xRPj4+evTRR3XkyBHT9j///FOTJk2SJPtfchdegfzOO+9Ikjp27FhgdVWvXl0pKSnatm2bfezw4cMOV75K59a3Xej8zZQvvC3PeaGhoWrQoIHmzZvn8Bf9b7/9pu+//96pf5m3bt1aY8aM0bvvvquQkJCL7ufh4WFKKT/99FP7Wr3zzjezF2tY8uOll17SX3/9pXnz5umdd95RlSpVFBMTc9HzeLXKly+vVq1aacaMGTp8+LBp+/l7Z0quOQ8XyquGKVOmmFLt82bOnKmsrCz7z9OmTVN2drbat29f4LUBcD6SRLit6tWra9GiRbr//vsVERHh8I0rv/zyiz799FP17t1bklS/fn3FxMRo5syZSk5OVsuWLbVhwwbNmzdPXbp0uejtVa5Gjx499NJLL+mee+7RU089pTNnzmjatGm68cYbHS4WGD16tNauXauOHTuqcuXKOnr0qN577z3dcMMNuvXWWy96/Lfeekvt27dXZGSk+vbtq/T0dE2ZMkX+/v6XnAa+VlarVUOHDr3sftHR0Ro9erT69Omj5s2ba/v27Vq4cKE9hTuvevXqCggI0PTp01W6dGn5+PioadOmqlq1ar7qiouL03vvvacRI0bYb8kzZ84ctWrVSsOGDdO4cePydbzLmTp1qm699VbVrVtX/fr1U7Vq1XTkyBHFx8frn3/+sd8HsbDPQ16io6O1YMEC+fv7q3bt2oqPj9cPP/ygoKCgPPc/e/as2rRpo+7du2v37t167733dOutt+ruu+++5loAuIALr6wGioQ//vjD6Nevn1GlShXD09PTKF26tNGiRQtjypQpDrcpycrKMkaNGmVUrVrVKFmypFGxYkVjyJAhDvsYxrlb4HTs2NH0OhfeeuVit8AxDMP4/vvvjTp16hienp5GeHi48eGHH5pugbNq1Sqjc+fORlhYmOHp6WmEhYUZDzzwgPHHH3+YXuPC26P88MMPRosWLQxvb2/Dz8/P6NSpk/H777877HP+9S68xc75253s37//oufUMBxvgXMxF7sFznPPPWeEhoYa3t7eRosWLYz4+Pg8b13zxRdfGLVr1zZKlCjh8D5btmxp3HTTTXm+5n+Pc+rUKaNy5cpGo0aNjKysLIf9nn32WcNqtRrx8fGXfA+SjAEDBuS57fy5+u8tcAzDMP7880+jV69eRkhIiFGyZEmjQoUKRnR0tPHZZ58Vynm42H+jF76XkydPGn369DHKli1r+Pr6Gm3btjV27dplVK5c2YiJiTG9zzVr1hj9+/c3ypQpY/j6+ho9e/Z0uNUSgOLFYhjc5RQAcPXmzp2rPn36aOPGjWrSpImrywFQQFiTCAAAABOaRAAAAJjQJAIAAMCEJhEAcE169+4twzBYjwgUkLVr16pTp04KCwuTxWLRsmXLLrrv448/LovFYrpF24kTJ9SzZ0/5+fkpICBAffv2VWpqar7qoEkEAAAoQtLS0lS/fn1NnTr1kvstXbpU69atU1hYmGlbz549tWPHDq1cuVLLly/X2rVr1b9//3zVwX0SAQAAipD27dtf9ib0//77rwYNGqTvvvvO9IUOO3fu1IoVKxzuODBlyhR16NBBb7/9dp5NZV5IEgEAAIqR3NxcPfzww3rhhRd00003mbbHx8crICDAYQlIVFSUrFar1q9ff8WvQ5IIAADgRJmZmaav+bTZbLLZbFd1vDfffFMlSpTQU089lef2xMRElS9f3mGsRIkSCgwMVGJi4hW/znXZJHo3HOjqElCIDq6d4OoSUIj8vEu6ugQATuLlwq7Emb3DS53LatSoUQ5jI0aMuKqvQt28ebMmTZqkLVu2yGKxFFCFeWO6GQAAwImGDBmilJQUh8eQIUOu6lg//fSTjh49qkqVKqlEiRIqUaKEDh48qOeee05VqlSRJIWEhOjo0aMOz8vOztaJEycUEhJyxa91XSaJAAAA+WJxXm52LVPLF3r44YcVFRXlMNa2bVs9/PDD6tOnjyQpMjJSycnJ2rx5sxo3bixJiouLU25urpo2bXrFr0WTCAAA4OSp2/xITU3V3r177T/v379fCQkJCgwMVKVKlRQUFOSwf8mSJRUSEqLw8HBJUkREhNq1a6d+/fpp+vTpysrK0sCBA9WjR48rvrJZYroZAACgSNm0aZMaNmyohg0bSpIGDx6shg0bavjw4Vd8jIULF6pWrVpq06aNOnTooFtvvVUzZ87MVx0kiQAAAE6cbs6vVq1ayTCMK97/wIEDprHAwEAtWrTomuooOmcEAAAARQZJIgAAQBFak1hUkCQCAADAhCQRAACgCK1JLCo4IwAAADAhSQQAAGBNoglNIgAAANPNJpwRAAAAmJAkAgAAMN1sQpIIAAAAE5JEAAAA1iSacEYAAABgQpIIAADAmkQTkkQAAACYkCQCAACwJtGEJhEAAIDpZhPaZgAAAJiQJAIAADDdbMIZAQAAgAlJIgAAAEmiCWcEAAAAJiSJAAAAVq5uvhBJIgAAAExIEgEAAFiTaEKTCAAAwM20TWibAQAAYEKSCAAAwHSzCWcEAAAAJiSJAAAArEk0IUkEAACACUkiAAAAaxJNOCMAAAAwIUkEAABgTaIJTSIAAADTzSacEQAAAJiQJAIAADDdbEKSCAAAABOSRAAAANYkmnBGAAAAYEKSCAAAwJpEE5JEAAAAmJAkAgAAsCbRhCYRAACAJtGEMwIAAAATkkQAAAAuXDEhSSziWjSqrs8mPqZ937+u9F/fVadW9S667+RXeyj913c18MFWDuNl/EppzusxOvLTWzq8dpymjXhQPt6eTq4cBSFhyya99OwAdWnXWrc1qaO1P65y2G4Yhj6Y/q46t22lNi0a65knH9Xffx10UbVwlo8XLVT7O+/QzQ3rqmeP+7R92zZXlwQn4vNGUUGTWMT5eNu0/Y9/9Uzs4kvud3frerqlbhUdOpps2jZnbIwiqocq+ol31e2p6bq1UQ1NHfagkypGQcpIT1eNmuEa/NKreW5fNG+2Pv94oZ4fMlwz5i6St5e3nhv0mDIzMwu5UjjLim+/0dvjYvXYkwP08adLFR5eS0881ldJSUmuLg1OwOftQhar8x7FVJGu/LfffnN1CS73/f9+16j3luvL1Rf/TTKsnL/eeek+9XllrrKycxy2hVcNVtsWN+nJ0Yu08beD+iVhnwa/+anua9tIoeX8nV0+rlGzFrep35NP6fbWUaZthmHok48WqFff/rqt1R2qUTNcr44eq6RjR/XTBYkjiq8F8+ao673d1eWebqpeo4aGjhglLy8vLVvyuatLgxPweaMoKXJN4unTpzVz5kzdcsstql+/vqvLKfIsFotmvdZLE+at0s59iabtTetV1clTZ7Tl97/sY3Hrdys319DNdSoXZqkoYIf//Ucnko6ryS2R9jFf39KKqFNPO7ZvdWFlKChZZ89q5+871CyyuX3MarWqWbPm2rb1VxdWBmfg83Yxi8V5j2KqyDSJa9euVUxMjEJDQ/X222/rjjvu0Lp161xdVpH3XJ87lZ2Tq6kf/Zjn9uAgPx07cdphLCcnVydOnVFwWb9CqBDOkpR0XJJUJijIYTwwMEgn/v82FG8nk08qJydHQRd8xkFBQTp+nM/4esPnjaLGpVc3JyYmau7cuZo1a5ZOnTql7t27KzMzU8uWLVPt2rWv6BiZmZmm9VdGbo4sVg9nlFykNIyoqAEPtFLzB990dSkAABRvxXjtoLO47Ix06tRJ4eHh2rZtmyZOnKhDhw5pypQp+T5ObGys/P39HR7ZRzY7oeKip0XD6iof6Ks/vhmt0xsn6fTGSaocFqQ3BnfVrq9HSZKOJJ1SucDSDs/z8LAq0K+Ujhw/5YqyUUCCgspKkk5esKD9xIkkBf7/bSjeygSUkYeHh+mihaSkJJUty2d8veHzdjGmm01c1iR+++236tu3r0aNGqWOHTvKw+Pqkr8hQ4YoJSXF4VEiuHEBV1s0Lfp6o27uHqumPd6wPw4dTdaE+T+o05NTJUnrt+1XGb9SahhR0f68VjffKKvVoo2/cauU4iy0wg0KDCqrzRv/b1lGWmqqdv62TTfVZT3v9aCkp6ciat+k9evi7WO5ublavz5e9eo3dGFlcAY+bxQ1Lptu/vnnnzVr1iw1btxYERERevjhh9WjR498H8dms8lmszmMXU9TzT7enqpesZz95yoVglTvxgo6eeqM/k48qRMpaQ77Z2Xn6MjxU9pz8Kgkaff+I/rufzs0ddiDeur1j1WyhIcmvNxdn363RYePpRTqe0H+nTlzRv/+/X8XHR3+91/t2b1Lfv7+Cg4JVfcHHta8WTN1Q8XKCq1QQR9Me1dB5crrtlZtXFg1CtLDMX007JWXdNNNdVSnbj19uGCe0tPT1eWerq4uDU7A5+06lmKc+DmLy5rEZs2aqVmzZpo4caIWL16s2bNna/DgwcrNzdXKlStVsWJFlS5d+vIHus41ql1Z33/wtP3ncc93kyQt+HKd+o/48IqO0eeVeZrwcnd9M2OQcnMNLVuVoOfGfeqUelGwdv/+m556/BH7z+9OGCdJahfdWa+OfF0Pxjyi9Ix0vTV2pFJPn1bdBo309uTppl+cUHy1a99BJ0+c0HvvTtbx48cUXitC7834QEFMP16X+LxRlFgMwzBcXcR5u3fv1qxZs7RgwQIlJyfrzjvv1Jdffpnv43g3HOiE6lBUHVw7wdUloBD5eZd0dQkAnMTLhZfT+tw7x2nHTvusT772X7t2rd566y1t3rxZhw8f1tKlS9WlSxdJUlZWloYOHapvvvlG+/btk7+/v6KiovTGG28oLCzMfowTJ05o0KBB+uqrr2S1WtWtWzdNmjRJvr6+V1xHkbqUJzw8XOPGjdM///yjjz76yNXlAAAAFLq0tDTVr19fU6dONW07c+aMtmzZomHDhmnLli1asmSJdu/erbvvvtthv549e2rHjh1auXKlli9frrVr16p///75qqNIJYkFhSTRvZAkuheSROD65dIk8T4nJomf5i9J/C+LxeKQJOZl48aNuuWWW3Tw4EFVqlRJO3fuVO3atbVx40Y1adJEkrRixQp16NBB//zzj0PieClFKkkEAABA/qSkpMhisSggIECSFB8fr4CAAHuDKElRUVGyWq1av379FR/XpTfTBgAAKAqceXVzXl/8kdfdWa5GRkaGXnrpJT3wwAPy8zv3TWqJiYkqX768w34lSpRQYGCgEhPNX+F7MSSJAADA7VksFqc98vrij9jY2GuuOSsrS927d5dhGJo2bVoBnAVHJIkAAABONGTIEA0ePNhh7FpTxPMN4sGDBxUXF2dPESUpJCRER48eddg/OztbJ06cUEhIyBW/Bk0iAABwe86cbi6oqeXzzjeIe/bs0erVqxUUFOSwPTIyUsnJydq8ebMaNz73LXRxcXHKzc1V06ZNr/h1aBIBAACKkNTUVO3du9f+8/79+5WQkKDAwECFhobq3nvv1ZYtW7R8+XLl5OTY1xkGBgbK09NTERERateunfr166fp06crKytLAwcOVI8ePa74ymaJJhEAAKBIfS3fpk2b1Lp1a/vP56eqY2JiNHLkSPsXjTRo0MDheatXr1arVq0kSQsXLtTAgQPVpk0b+820J0+enK86aBIBAACKkFatWulSt7G+kltcBwYGatGiRddUB00iAABA0QkSiwxugQMAAAATkkQAAOD2itKaxKKCJBEAAAAmJIkAAMDtkSSa0SQCAAC3R5NoxnQzAAAATEgSAQCA2yNJNCNJBAAAgAlJIgAAAEGiCUkiAAAATEgSAQCA22NNohlJIgAAAExIEgEAgNsjSTSjSQQAAG6PJtGM6WYAAACYkCQCAAAQJJqQJAIAAMCEJBEAALg91iSakSQCAADAhCQRAAC4PZJEM5JEAAAAmJAkAgAAt0eSaEaTCAAA3B5NohnTzQAAADAhSQQAACBINCFJBAAAgAlJIgAAcHusSTQjSQQAAIAJSSIAAHB7JIlmJIkAAAAwIUkEAABujyTRjCYRAACAHtGE6WYAAACYkCQCAAC3x3SzGUkiAAAATEgSAQCA2yNJNCNJBAAAgAlJIgAAcHskiWYkiQAAADAhSQQAAG6PJNGMJhEAAIAe0YTpZgAAAJhcl0ni6s9ec3UJKER3vPmjq0tAIdowPMrVJaAQWa3EOygcTDebkSQCAADA5LpMEgEAAPKDJNGMJBEAAAAmJIkAAMDtESSakSQCAADAhCQRAAC4PdYkmtEkAgAAt0ePaMZ0MwAAAExIEgEAgNtjutmMJBEAAAAmJIkAAMDtESSakSQCAAAUIWvXrlWnTp0UFhYmi8WiZcuWOWw3DEPDhw9XaGiovL29FRUVpT179jjsc+LECfXs2VN+fn4KCAhQ3759lZqamq86aBIBAIDbs1otTnvkV1pamurXr6+pU6fmuX3cuHGaPHmypk+frvXr18vHx0dt27ZVRkaGfZ+ePXtqx44dWrlypZYvX661a9eqf//++aqD6WYAAIAipH379mrfvn2e2wzD0MSJEzV06FB17txZkjR//nwFBwdr2bJl6tGjh3bu3KkVK1Zo48aNatKkiSRpypQp6tChg95++22FhYVdUR0kiQAAwO1ZLM57FKT9+/crMTFRUVFR9jF/f381bdpU8fHxkqT4+HgFBATYG0RJioqKktVq1fr166/4tUgSAQCA23PmLXAyMzOVmZnpMGaz2WSz2fJ9rMTERElScHCww3hwcLB9W2JiosqXL++wvUSJEgoMDLTvcyVIEgEAAJwoNjZW/v7+Do/Y2FhXl3VZJIkAAMDtOfMWOEOGDNHgwYMdxq4mRZSkkJAQSdKRI0cUGhpqHz9y5IgaNGhg3+fo0aMOz8vOztaJEyfsz78SJIkAAABOZLPZ5Ofn5/C42iaxatWqCgkJ0apVq+xjp06d0vr16xUZGSlJioyMVHJysjZv3mzfJy4uTrm5uWratOkVvxZJIgAAcHtF6Wv5UlNTtXfvXvvP+/fvV0JCggIDA1WpUiU988wzeu2111SzZk1VrVpVw4YNU1hYmLp06SJJioiIULt27dSvXz9Nnz5dWVlZGjhwoHr06HHFVzZLNIkAAABFyqZNm9S6dWv7z+enqmNiYjR37ly9+OKLSktLU//+/ZWcnKxbb71VK1askJeXl/05Cxcu1MCBA9WmTRtZrVZ169ZNkydPzlcdFsMwjIJ5S0XHuj+TXV0CCtGjcza6ugQUog3Doy6/E64bV3MjYhRfXi6MruqPWHX5na7S1lFtnHZsZ2JNIgAAAEyYbgYAAG6vCC1JLDJoEgEAgNsrSheuFBVMNwMAAMCEJBEAALg9gkQzkkQAAACYkCQCAAC3x5pEM5JEAAAAmJAkAgAAt0eQaEaSCAAAABOSRAAA4PZYk2hGkggAAAATkkQAAOD2CBLNaBIBAIDbY7rZjOlmAAAAmJAkAgAAt0eQaEaSCAAAABOSRAAA4PZYk2hGkggAAAATkkQAAOD2CBLNSBIBAABgQpIIAADcHmsSzWgSAQCA26NHNGO6GQAAACYkiQAAwO0x3WxGkggAAAATkkQAAOD2SBLNSBIBAABgQpIIAADcHkGiGU1iMbPq688V9/USHT9ySJJUoXI1dX6gr+rf3NxhP8MwNH74s9q+OV5PDR2nxs1buqJc5FPjKgHqc2sV1Q7zU3k/m55amKC4nccc9hnQprrubVJBpb1K6Ne/kjXmy136K+mMfbufdwm9El1LrcLLKdcw9MPvRxX79W6ln80p7LeDazTrgxmK+2GlDuzfJ5uXl+rXb6inn31OVapWc3VpcKKPFy3UvDmzdPz4Md0YXksvvzJMdevVc3VZcENMNxczgWXLq3ufJzVq8jyNmjRPtes30aQxL+ifg/sc9vtu2cf8VlQMeZf00O7E03r9q515bn/ktirq2ayiRn+xUw9O36D0szmaEdNQniX+74/ym/fVVY3yvuo3d7MGfJigxpXLaGTniMJ6CyhAWzZt1P09HtT8hYs1beZsZWdn64nHHlX6mTOXfzKKpRXffqO3x8XqsScH6ONPlyo8vJaeeKyvkpKSXF3adc9isTjtUVzRJBYzDZvepvo3t1BIhUoKuaGS7o15Ql5epfTnrt/s+xz88w+tWLJQfZ8Z5sJKcTV+3pOkKT/8qVUXpIfnPdy8kmb+uF+rdx3TH0dS9cpnO1S+tE1tIspJkqqV89FtN5bViKW/a/s/p/TrwWSN/XqX2tcNUbnStsJ8KygAU6d/oLu7dFX1GjUVHl5Lo16LVeLhQ/r99x2uLg1OsmDeHHW9t7u63NNN1WvU0NARo+Tl5aVlSz53dWnXPYvFeY/iiiaxGMvNydG6Nd8rMyNdNSLqSJIyMzI0fdww9XryBQUEBrm4QhSkG8p4q1xpm+L//L9EITUzW9v+OaX6FQMkSfUr+islPUs7Dp2y77PuzxPKNQzVu8GvsEtGAUtNPS1J8vf3d3ElcIass2e18/cdahb5f8uHrFarmjVrrm1bf3VhZXBXRWJNYlJSkoKCzjU0f//9t95//32lp6fr7rvv1m233ebi6oqev/fv1ZjnHlXW2bPy8vbWU8PeVIVK59YoLXp/gmpE1FOjSNYgXm/K+npKkpJSzzqMJ6Vmqmzpc9vKlvbUiQu25+QaSknPVlmSxGItNzdXb785Vg0aNlKNmje6uhw4wcnkk8rJybH/fXheUFCQ9u/fd5FnoaAU52lhZ3Fpk7h9+3Z16tRJf//9t2rWrKmPP/5Y7dq1U1pamqxWqyZMmKDPPvtMXbp0uegxMjMzlZmZ6TB2NjNTnrbr9y/E0Bsqa8y7C3QmLVUbf47T++NHa8i4aTpy6B/t3LpJo6cscHWJAApY7OujtXfvHs2Zt8jVpQBwEy6dbn7xxRdVt25drV27Vq1atVJ0dLQ6duyolJQUnTx5Uo899pjeeOONSx4jNjZW/v7+Do/50ycU0jtwjRIlSyo4rKKq1oxQ9z4DVLFaTX3/xWLt3LpJRw//qyfui1Kf6ObqE31uymLK2JcV+9ITLq4a1+r4/08Ig/5/onhekK9Nx0+f23b89FkFXrDdw2qRv3cJHT/t+MsUio83Xh+tn9b8qPdnzVdwSIiry4GTlAkoIw8PD9NFKklJSSpbtqyLqnIfrEk0c2mSuHHjRsXFxalevXqqX7++Zs6cqSeffFJW67neddCgQWrWrNkljzFkyBANHjzYYSzhn3Sn1VwUGbm5ys7K0j09+6tl284O21598kE92O8ZNWzKtH1x98/JdB07nalm1YO0OzFVkuRj81C9G/z0yYa/JUlb/06Rv3dJ1Q4rrd8PnVu/1rRaGVktFm3759RFj42iyTAMvTl2jOLiftD7s+erwg03uLokOFFJT09F1L5J69fF6442UZLOLTNYvz5ePR54yMXVwR25tEk8ceKEQv7/b8W+vr7y8fFRmTJl7NvLlCmj06dPX/IYNptNtgumlj1tuQVfbBHxyZypqtekuYLKByvjzBnF//iddm3foufHTFJAYFCeF6sElQtRuZAwF1SL/PL29FClQG/7zxXKeCs8xFcp6dlKTMnQgl/+Uv9WVXUw6Yz+PZmugW2q6+jpTPvV0PuOpemnP45rZJfaGv3FTpX0sOiV6Fr6dnuijpEkFjuxr4/Wt98s14RJU+Xj46Pjx899zr6+peXl5eXi6uAMD8f00bBXXtJNN9VRnbr19OGCeUpPT1eXe7q6urTrnrU4R35O4vILVy5cKMrC0Us7nXJS748fpeQTx+Xt46uKVWvo+TGTVKdRU1eXhgJQp4Kf5vRtYv/5pQ7hkqRlWw5p6JIdmv3TAXl7emhk5wiV9iqhLX8l6/F5v+ps9v/9YvTSp9v1anQtzXqk8bmbae84qrFf7y7094Jr9+nijyRJ/R7p5TA+asxY3d2FpuF61K59B508cULvvTtZx48fU3itCL034wMFMd0MF7AYhmG46sWtVqvat29vTwK/+uor3XHHHfLx8ZF07qKUFStWKCcnf98Use7P5IIuFUXYo3M2uroEFKINw6NcXQIKkdVKcOBOvFwYXd01dZ3Tjv39gEsvnSuqXJokxsTEOPz80EPmNRe9evUyjQEAABQkZjLNXNokzpkzx5UvDwAAgItw+ZpEAAAAV2NlgxlfywcAAAATkkQAAOD2WJNoRpIIAAAAE5JEAADg9ggSzUgSAQAAYEKSCAAA3J5FRIkXokkEAABuj1vgmDHdDAAAABOSRAAA4Pa4BY4ZSSIAAABMSBIBAIDbI0g0I0kEAACACU0iAABwe1aLxWmP/MjJydGwYcNUtWpVeXt7q3r16hozZowMw7DvYxiGhg8frtDQUHl7eysqKkp79uwp6FNCkwgAAFBUvPnmm5o2bZreffdd7dy5U2+++abGjRunKVOm2PcZN26cJk+erOnTp2v9+vXy8fFR27ZtlZGRUaC1sCYRAAC4vaKyJvGXX35R586d1bFjR0lSlSpV9NFHH2nDhg2SzqWIEydO1NChQ9W5c2dJ0vz58xUcHKxly5apR48eBVYLSSIAAHB7FovFaY/8aN68uVatWqU//vhDkrR161b9/PPPat++vSRp//79SkxMVFRUlP05/v7+atq0qeLj4wvuhIgkEQAAwKkyMzOVmZnpMGaz2WSz2Uz7vvzyyzp16pRq1aolDw8P5eTk6PXXX1fPnj0lSYmJiZKk4OBgh+cFBwfbtxUUkkQAAOD2LBbnPWJjY+Xv7+/wiI2NzbOOTz75RAsXLtSiRYu0ZcsWzZs3T2+//bbmzZtXyGeEJBEAAMCphgwZosGDBzuM5ZUiStILL7ygl19+2b62sG7dujp48KBiY2MVExOjkJAQSdKRI0cUGhpqf96RI0fUoEGDAq2bJBEAALg9Z94Cx2azyc/Pz+FxsSbxzJkzslod2zMPDw/l5uZKkqpWraqQkBCtWrXKvv3UqVNav369IiMjC/SckCQCAAAUEZ06ddLrr7+uSpUq6aabbtKvv/6qd955R4888oikcxfYPPPMM3rttddUs2ZNVa1aVcOGDVNYWJi6dOlSoLXQJAIAALdXRO6AoylTpmjYsGF68skndfToUYWFhemxxx7T8OHD7fu8+OKLSktLU//+/ZWcnKxbb71VK1askJeXV4HWYjH+ewvv68S6P5NdXQIK0aNzNrq6BBSiDcOjLr8TrhtWa1H5qxuFwcuF0VWPeb867dgfxzR02rGdiSQRAAC4vfzez9Ad0CQCAAC3R2htxtXNAAAAMCFJBAAAbo/pZjOSRAAAAJiQJAIAALdHkGhGkggAAAATkkQAAOD2WJNoRpIIAAAAE5JEAADg9rhPohlNIgAAcHtMN5sx3QwAAAATkkQAAOD2yBHNripJ/Omnn/TQQw8pMjJS//77ryRpwYIF+vnnnwu0OAAAALhGvpvEzz//XG3btpW3t7d+/fVXZWZmSpJSUlI0duzYAi8QAADA2awWi9MexVW+m8TXXntN06dP1/vvv6+SJUvax1u0aKEtW7YUaHEAAABwjXyvSdy9e7duv/1207i/v7+Sk5MLoiYAAIBCVYwDP6fJd5IYEhKivXv3msZ//vlnVatWrUCKAgAAgGvlu0ns16+fnn76aa1fv14Wi0WHDh3SwoUL9fzzz+uJJ55wRo0AAABOZbFYnPYorvI93fzyyy8rNzdXbdq00ZkzZ3T77bfLZrPp+eef16BBg5xRIwAAAApZvptEi8WiV199VS+88IL27t2r1NRU1a5dW76+vs6oDwAAwOmKceDnNFd9M21PT0/Vrl27IGsBAABwieJ8qxpnyXeT2Lp160vOr8fFxV1TQQAAAHC9fDeJDRo0cPg5KytLCQkJ+u233xQTE1NQdQEAABQagkSzfDeJEyZMyHN85MiRSk1NveaCAAAA4HpX9d3NeXnooYc0e/bsgjocAABAoeEWOGYF1iTGx8fLy8uroA4HAAAAF8r3dHPXrl0dfjYMQ4cPH9amTZs0bNiwAivsWjSoHODqElCI4oe2cXUJKET/nEx3dQkoRBUDS7m6BLiJAkvNriP5bhL9/f0dfrZarQoPD9fo0aN11113FVhhAAAAcJ18NYk5OTnq06eP6tatqzJlyjirJgAAgEJVnNcOOku+0lUPDw/dddddSk5OdlI5AAAAhc9qcd6juMr3FHydOnW0b98+Z9QCAACAIiLfTeJrr72m559/XsuXL9fhw4d16tQphwcAAEBxQ5JodsVrEkePHq3nnntOHTp0kCTdfffdDvP3hmHIYrEoJyen4KsEAABAobriJnHUqFF6/PHHtXr1amfWAwAAUOi4cMXsiptEwzAkSS1btnRaMQAAACga8nULHLpsAABwPSrOawedJV9N4o033njZRvHEiRPXVBAAAABcL19N4qhRo0zfuAIAAFDcMVlqlq8msUePHipfvryzagEAAHAJK12iyRXfJ5H1iAAAAO4j31c3AwAAXG/y/e0ibuCKm8Tc3Fxn1gEAAIAiJF9rEgEAAK5HrKozI10FAACACUkiAABwe1zdbEaSCAAAABOSRAAA4PYIEs1oEgEAgNvju5vNmG4GAACACUkiAABwe1y4YkaSCAAAABOSRAAA4PYIEs1IEgEAAGBCkggAANweVzebkSQCAAAUIf/++68eeughBQUFydvbW3Xr1tWmTZvs2w3D0PDhwxUaGipvb29FRUVpz549BV4HTSIAAHB7Fif+kx8nT55UixYtVLJkSX377bf6/fffNX78eJUpU8a+z7hx4zR58mRNnz5d69evl4+Pj9q2bauMjIwCPSdMNwMAALdXVKab33zzTVWsWFFz5syxj1WtWtX+74ZhaOLEiRo6dKg6d+4sSZo/f76Cg4O1bNky9ejRo8BqIUkEAAAoIr788ks1adJE9913n8qXL6+GDRvq/ffft2/fv3+/EhMTFRUVZR/z9/dX06ZNFR8fX6C10CQCAAC3Z7U475GZmalTp045PDIzM/OsY9++fZo2bZpq1qyp7777Tk888YSeeuopzZs3T5KUmJgoSQoODnZ4XnBwsH1bgZ2TAj0aAAAAHMTGxsrf39/hERsbm+e+ubm5atSokcaOHauGDRuqf//+6tevn6ZPn17IVdMkAgAAyGKxOO0xZMgQpaSkODyGDBmSZx2hoaGqXbu2w1hERIT++usvSVJISIgk6ciRIw77HDlyxL6toNAkAgAAOJHNZpOfn5/Dw2az5blvixYttHv3boexP/74Q5UrV5Z07iKWkJAQrVq1yr791KlTWr9+vSIjIwu0bq5uBgAAbq+oXN387LPPqnnz5ho7dqy6d++uDRs2aObMmZo5c6akc4nnM888o9dee001a9ZU1apVNWzYMIWFhalLly4FWgtNIgAAQBFx8803a+nSpRoyZIhGjx6tqlWrauLEierZs6d9nxdffFFpaWnq37+/kpOTdeutt2rFihXy8vIq0FoshmEYBXrEIiAj29UVoDBlZee6ugQUosMpBXuzWBRtFQNLuboEFCLvkq577XfW7nPasQffXs1px3YmkkQAAOD2rJYiMt9chHDhCgAAAExIEgEAgNsrKheuFCUkiQAAADAhSQQAAG6PJYlmJIkAAAAwIUkEAABuzyqixAuRJAIAAMCEJBEAALg91iSa0SQCAAC3xy1wzJhuBgAAgAlJIgAAcHt8LZ8ZSSIAAABMaBKvEx8vWqj2d96hmxvWVc8e92n7tm2uLgmFYO6s99WkfoTGjxvr6lJwjT79cJae7d9T3du20EN336HXXnlW//x1wGGfFV9+riFPParu7W5Vp9sbKvX0adcUC6fYvGmjnhrwuO5sfasa1AlX3KofXF2SW7FYnPcormgSrwMrvv1Gb4+L1WNPDtDHny5VeHgtPfFYXyUlJbm6NDjRjt+2a8lni1XzxnBXl4IC8FvCFnW85369NX2+xrwzTTnZ2Rr+3BPKSE+375OZkaFGtzTXfQ894sJK4Szp6Wd0Y3i4hrw6wtWlAJJc2CTGxcWpdu3aOnXqlGlbSkqKbrrpJv30008uqKz4WTBvjrre211d7umm6jVqaOiIUfLy8tKyJZ+7ujQ4yZkzaRo25AW9OmK0Svv5ubocFIBRb09VVPu7VblqdVWtEa5nXhmlY0cStXf37/Z9OnfvqfseekS1bqrnwkrhLLfe1lIDn3pWd0Td6epS3JLVYnHao7hyWZM4ceJE9evXT355/AXn7++vxx57TO+8844LKitess6e1c7fd6hZZHP7mNVqVbNmzbVt668urAzO9ObYMWpxe0s1bdb88jujWEpLTZUklfbzd3ElANyVy5rErVu3ql27dhfdftddd2nz5s2XPU5mZqZOnTrl8MjMzCzIUou0k8knlZOTo6CgIIfxoKAgHT9+3EVVwZm++/Zr7dr5uwY+NdjVpcBJcnNz9f6UtxVRt4EqV6vh6nIAt8CaRDOXNYlHjhxRyZIlL7q9RIkSOnbs2GWPExsbK39/f4fHW2/GFmSpQJGRmHhY48fF6rXYt2Sz2VxdDpxk+oRY/bV/r14c8YarSwHchtWJj+LKZfdJrFChgn777TfVqJH3b8nbtm1TaGjoZY8zZMgQDR7smKgYHu7zl2eZgDLy8PAwXaSSlJSksmXLuqgqOMuu33foxIkkPdSjm30sJydHv27epE8+XqRfNm6Vh4eHCyvEtZo+4Q1t/OUnxU6ZpbLlg11dDgA35rImsUOHDho2bJjatWsnLy8vh23p6ekaMWKEoqOjL3scm81mSlQysgu01CKtpKenImrfpPXr4nVHmyhJ56aq1q+PV48HHnJxdShoNzeN1MeffeEwNnrEq6pcpapi+jxKg1iMGYahGRPfVPxPcYqd9L5Cwiq4uiTArViK87ywk7isSRw6dKiWLFmiG2+8UQMHDlR4+LnbeOzatUtTp05VTk6OXn31VVeVV6w8HNNHw155STfdVEd16tbThwvmKT09XV3u6erq0lDAfHx8VKPmjQ5jXt7eCggIMI2jeJk2IVZrf/hWr46dIO9SPjqZdG5NcSlfX9ls536RPpl0XCdPJOnQv39Jkg7u2yPvUj4qFxzCBS7XgTNn0vTXX3/Zf/7333+0a9dO+fv7KzQ0zIWVwV25rEkMDg7WL7/8oieeeEJDhgyRYRiSznXybdu21dSpUxUczFTLlWjXvoNOnjih996drOPHjym8VoTem/GBgphuBoqNb5d9Kkl65al+DuNPDxmlqPZ3n9vni8/00dwZ9m0vD+pr2gfF147fflO/R3rZfx4/7tz6+k6d79GY11mf6mzkiGYW43x35kInT57U3r17ZRiGatasqTJlylzT8dxpuhlSVnauq0tAITqckuHqElCIKgaWcnUJKETeF7+e1enmb/rbacfu1aSi047tTC5LEv+rTJkyuvnmm11dBgAAcFPF+abXzlKcr8wGAACAkxSJJBEAAMCVyBHNaBIBAIDbY7bZjOlmAAAAmJAkAgAAt8fNtM1IEgEAAGBCkggAANweqZkZ5wQAAAAmJIkAAMDtsSbRjCQRAAAAJiSJAADA7ZEjmpEkAgAAwIQkEQAAuD3WJJrRJAIAALfH1KoZ5wQAAAAmJIkAAMDtMd1sRpIIAAAAE5JEAADg9sgRzUgSAQAAYEKSCAAA3B5LEs1IEgEAAGBCkggAANyelVWJJjSJAADA7THdbMZ0MwAAAExIEgEAgNuzMN1sQpIIAAAAE5JEAADg9liTaEaSCAAAABOSRAAA4Pa4BY4ZSSIAAABMaBIBAIDbs1ic97gWb7zxhiwWi5555hn7WEZGhgYMGKCgoCD5+vqqW7duOnLkyLW9UB5oEgEAgNsrik3ixo0bNWPGDNWrV89h/Nlnn9VXX32lTz/9VGvWrNGhQ4fUtWvXazwDZjSJAAAARUxqaqp69uyp999/X2XKlLGPp6SkaNasWXrnnXd0xx13qHHjxpozZ45++eUXrVu3rkBroEkEAABuz+LEf67GgAED1LFjR0VFRTmMb968WVlZWQ7jtWrVUqVKlRQfH39N5+BCXN0MAADgRJmZmcrMzHQYs9lsstlsee7/8ccfa8uWLdq4caNpW2Jiojw9PRUQEOAwHhwcrMTExAKrWSJJBAAAkNXivEdsbKz8/f0dHrGxsXnW8ffff+vpp5/WwoUL5eXlVchnwRFJIgAAgBMNGTJEgwcPdhi7WIq4efNmHT16VI0aNbKP5eTkaO3atXr33Xf13Xff6ezZs0pOTnZIE48cOaKQkJACrZsmEQAAuL2rXTt4JS41tXyhNm3aaPv27Q5jffr0Ua1atfTSSy+pYsWKKlmypFatWqVu3bpJknbv3q2//vpLkZGRBVo3TSIAAEARUbp0adWpU8dhzMfHR0FBQfbxvn37avDgwQoMDJSfn58GDRqkyMhINWvWrEBroUkEAABu71pvel2YJkyYIKvVqm7duikzM1Nt27bVe++9V+CvYzEMwyjwo7pYRrarK0BhysrOdXUJKESHUzJcXQIKUcXAUq4uAYXIu6TrXvvH3SecduxW4YFOO7YzcXUzAAAATJhuBgAAbs9ajKabCwtJIgAAAExIEgEAgNtz5i1wiiuSRAAAAJiQJAIAALdXnG6BU1hIEgEAAGBCkggAANweQaIZTSIAAHB7VuabTZhuBgAAgAlJIoq9szl8LZ87uaGMt6tLQCFav895X5WGoseVX19HjmhGkggAAAATkkQAAACiRBOSRAAAAJiQJAIAALfH1/KZkSQCAADAhCQRAAC4PW6TaEaTCAAA3B49ohnTzQAAADAhSQQAACBKNCFJBAAAgAlJIgAAcHvcAseMJBEAAAAmJIkAAMDtcQscM5JEAAAAmJAkAgAAt0eQaEaTCAAAQJdownQzAAAATEgSAQCA2+MWOGYkiQAAADAhSQQAAG6PW+CYkSQCAADAhCQRAAC4PYJEM5JEAAAAmJAkAgAAECWa0CQCAAC3xy1wzJhuBgAAgAlJIgAAcHvcAseMJBEAAAAmJIkAAMDtESSakSQCAADAhCQRAACAKNGEJBEAAAAmJIkAAMDtcZ9EM5JEAAAAmJAkAgAAt8d9Es1oEgEAgNujRzRjuhkAAAAmJIkAAABEiSYkiQAAADAhSQQAAG6PW+CYkSQCAADAhCQRAAC4PW6BY0aSCAAAABOSRAAA4PYIEs1IEgEAACxOfORDbGysbr75ZpUuXVrly5dXly5dtHv3bod9MjIyNGDAAAUFBcnX11fdunXTkSNHruptXwpNIgAAQBGxZs0aDRgwQOvWrdPKlSuVlZWlu+66S2lpafZ9nn32WX311Vf69NNPtWbNGh06dEhdu3Yt8FoshmEYBX5UF8vIdnUFKExpmXzg7sS7pIerS0Ah2rD/pKtLQCFqFR7ostfecyTdaceuGex91c89duyYypcvrzVr1uj2229XSkqKypUrp0WLFunee++VJO3atUsRERGKj49Xs2bNCqpskkQAAICiKiUlRZIUGHiugd68ebOysrIUFRVl36dWrVqqVKmS4uPjC/S1uXAFAAC4PWfeAiczM1OZmZkOYzabTTab7ZLPy83N1TPPPKMWLVqoTp06kqTExER5enoqICDAYd/g4GAlJiYWaN0kiQAAAE4UGxsrf39/h0dsbOxlnzdgwAD99ttv+vjjjwuhSjOSRAAA4PaceQucIUOGaPDgwQ5jl0sRBw4cqOXLl2vt2rW64YYb7OMhISE6e/askpOTHdLEI0eOKCQkpEDrpkm8Tny8aKHmzZml48eP6cbwWnr5lWGqW6+eq8tCAcvJydGsGVP13TfLlZR0XGXLlVfHTp3V+9HHZeHrAq47sz6YobgfVurA/n2yeXmpfv2GevrZ51SlajVXl4YCsOabJVrz7RIlHT0sSQqtVE3RPR5RncaRkqSUk0n6fM672pmwQRnpZxRcoZI6dO+tRs1bu7JsXIUrmVo+zzAMDRo0SEuXLtWPP/6oqlWrOmxv3LixSpYsqVWrVqlbt26SpN27d+uvv/5SZGRkgdZNk3gdWPHtN3p7XKyGjhilunXra+GCeXrisb76YvkKBQUFubo8FKAP587S0s8Wa+iosapWvYZ2/v6bxo4cKh/f0ur+wEOuLg8FbMumjbq/x4O6qU5dZefk6N1JE/TEY49qybLl8i5VytXl4RoFlC2ne2KeVPmwipJhKD7uG733+osaOnGewipV05wJo5WedlpPDh0nX78AbVjzvWaOG6pXxs9Wperhri7/+lNEfs8eMGCAFi1apC+++EKlS5e2rzP09/eXt7e3/P391bdvXw0ePFiBgYHy8/PToEGDFBkZWaBXNkusSbwuLJg3R13v7a4u93RT9Ro1NHTEKHl5eWnZks9dXRoK2PatCbqt5R1qcVtLhYZV0B1RbXVLs+b6/bftri4NTjB1+ge6u0tXVa9RU+HhtTTqtVglHj6k33/f4erSUADq33Kb6jZpruCwigquUEldHn5cNi9v7dv1myRp367tah19n6reeJPKhVRQx/v7qJSPr/76c/dljoyrYXHiP/kxbdo0paSkqFWrVgoNDbU/Fi9ebN9nwoQJio6OVrdu3XT77bcrJCRES5YsKehT4vomMTc3V7Nnz1Z0dLTq1KmjunXr6u6779b8+fN1Hd7CscBlnT2rnb/vULPI5vYxq9WqZs2aa9vWX11YGZyhbv0G2rRhnf46eECStOePXdqa8KsiW9zm2sJQKFJTT0s6lyjg+pKbk6ONa1fqbEaGqtWqK0mqVquuNv30g9JOpyg3N1cb165U1tmzurFOQxdXC2cyDCPPR+/eve37eHl5aerUqTpx4oTS0tK0ZMmSAl+PKLl4utkwDN1999365ptvVL9+fdWtW1eGYWjnzp3q3bu3lixZomXLlrmyxCLvZPJJ5eTkmKaVg4KCtH//PhdVBWd5uM+jSktL1QNdo2X18FBuTo4eG/C02naIdnVpcLLc3Fy9/eZYNWjYSDVq3ujqclBA/j2wV2++2F9ZZ8/K5u2tx195Q2GVzq1B6//ia3r/rWEa3LOdrB4e8rR56YlX3jg3PY0Cx7JuM5c2iXPnztXatWu1atUqtW7tuBA3Li5OXbp00fz589WrV6+LHiOvew8ZHle+QBQoTlatXKHvv/1aI8eOU7VqNfTH7l2aNP4NlS1XTh06dXF1eXCi2NdHa+/ePZozb5GrS0EBCq5QWUMnzlP6mTRt+V+c5k4co+fGvqewSlX1xcKZOpN2Ws+MmSxfvwAlrFurmeOG6oXYaapQpYarS4cbcOl080cffaRXXnnF1CBK0h133KGXX35ZCxcuvOQx8rr30FtvXv7eQ9eLMgFl5OHhoaSkJIfxpKQklS1b1kVVwVmmThyvh3v31Z1tO6h6zRvVPvpu3d+zl+bP+cDVpcGJ3nh9tH5a86PenzVfwU6YUoLrlChZUuXDKqpyjVq6J+ZJ3VC1huK+Wqxjh//Rj19/ppinXlVE/ZtVsWpNdXqgryrXqKUfv2G9uTNYnPgorlzaJG7btk3t2rW76Pb27dtr69atlzzGkCFDlJKS4vB44aUhBV1qkVXS01MRtW/S+nX/91U8ubm5Wr8+XvXqs27lepORkS6L1fGPrYfVQ0ZurosqgjMZhqE3Xh+tuLgfNGPWXFX4z73ScH0ycg1lZ2XpbGaGJMlicfzzbrV6KDeX9fooHC6dbj5x4oSCg4Mvuj04OFgnT176y93zuvdQRnaBlFdsPBzTR8NeeUk33VRHderW04cL5ik9PV1d7unq6tJQwG69vZXmzZqp4JBQVateQ3/s2qmPP5ynjp3vcXVpcILY10fr22+Wa8KkqfLx8dHx48ckSb6+peXl5eXi6nCtls57Tzc1jlRguRBlpqdpw5rv9cdvW/TUyIkKuaGKyofeoA+nvql7Hxko39L+Sli3VjsTNmjAsLddXfr1qThHfk5iMVx4CbGHh4cSExNVrly5PLcfOXJEYWFhysnJyddx3a1JlKSPFn5ov5l2eK0IvfTKUNWrV9/VZRWKtEz3+cDT0tL0/nuTtWb1Kp08eUJly5XXnW3b65H+T6hkSU9Xl1covEt6uLqEQtOwbq08x0eNGau7u7jHL4Eb9l86KCjO5k9+Xbu2bVLKiSR5+/iqQpXqatv1YdVueIsk6cihv7V03nva+/tWZWakq3zoDbrzngfVrHV7F1fuPK3CA1322geSMpx27CpBxfOXOpc2iVarVe3bt7/oRSaZmZlasWIFTSIuyZ2aRLhXk4jru0mEmSubxINJmZff6SpVDiqeF9O6dLo5Jibmsvtc6spmAACAgsAtcMxc2iTOmTPHlS8PAACAi+C7mwEAgNsjSDRz+dfyAQAAoOghSQQAAG6PNYlmJIkAAAAwIUkEAABgVaIJSSIAAABMSBIBAIDbY02iGU0iAABwe/SIZkw3AwAAwIQkEQAAuD2mm81IEgEAAGBCkggAANyehVWJJiSJAAAAMCFJBAAAIEg0IUkEAACACUkiAABwewSJZjSJAADA7XELHDOmmwEAAGBCkggAANwet8AxI0kEAACACUkiAAAAQaIJSSIAAABMSBIBAIDbI0g0I0kEAACACUkiAABwe9wn0YwmEQAAuD1ugWPGdDMAAABMSBIBAIDbY7rZjCQRAAAAJjSJAAAAMKFJBAAAgAlrEgEAgNtjTaIZSSIAAABMSBIBAIDb4z6JZjSJAADA7THdbMZ0MwAAAExIEgEAgNsjSDQjSQQAAIAJSSIAAABRoglJIgAAAExIEgEAgNvjFjhmJIkAAAAwIUkEAABuj/skmpEkAgAAwIQkEQAAuD2CRDOaRAAAALpEE6abAQAAipipU6eqSpUq8vLyUtOmTbVhw4ZCr4EmEQAAuD2LE//Jr8WLF2vw4MEaMWKEtmzZovr166tt27Y6evSoE975xdEkAgAAFCHvvPOO+vXrpz59+qh27dqaPn26SpUqpdmzZxdqHTSJAADA7Vksznvkx9mzZ7V582ZFRUXZx6xWq6KiohQfH1/A7/rSuHAFAADAiTIzM5WZmekwZrPZZLPZTPseP35cOTk5Cg4OdhgPDg7Wrl27nFrnha7LJtHrunxXl5aZmanY2FgNGTIkz//ormdeJdzvA3fnz9sdufPn3So80NUlFDp3/rxdyZm9w8jXYjVq1CiHsREjRmjkyJHOe9ECYDEMw3B1Ebh2p06dkr+/v1JSUuTn5+fqcuBkfN7uhc/bvfB5X3/ykySePXtWpUqV0meffaYuXbrYx2NiYpScnKwvvvjC2eXasSYRAADAiWw2m/z8/BweF0uJPT091bhxY61atco+lpubq1WrVikyMrKwSpZ0nU43AwAAFFeDBw9WTEyMmjRpoltuuUUTJ05UWlqa+vTpU6h10CQCAAAUIffff7+OHTum4cOHKzExUQ0aNNCKFStMF7M4G03idcJms2nEiBEscnYTfN7uhc/bvfB5Q5IGDhyogQMHurQGLlwBAACACReuAAAAwIQmEQAAACY0iQAAADChSbxOxMfHy8PDQx07dnR1KXCi3r17y2Kx2B9BQUFq166dtm3b5urS4CSJiYkaNGiQqlWrJpvNpooVK6pTp04O91BD8fffP9slS5ZUcHCw7rzzTs2ePVu5ubmuLg9uiibxOjFr1iwNGjRIa9eu1aFDh1xdDpyoXbt2Onz4sA4fPqxVq1apRIkSio6OdnVZcIIDBw6ocePGiouL01tvvaXt27drxYoVat26tQYMGODq8lDAzv/ZPnDggL799lu1bt1aTz/9tKKjo5Wdne3q8uCGuAXOdSA1NVWLFy/Wpk2blJiYqLlz5+qVV15xdVlwEpvNppCQEElSSEiIXn75Zd122206duyYypUr5+LqUJCefPJJWSwWbdiwQT4+Pvbxm266SY888ogLK4Mz/PfPdoUKFdSoUSM1a9ZMbdq00dy5c/Xoo4+6uEK4G5LE68Ann3yiWrVqKTw8XA899JBmz54t7mzkHlJTU/Xhhx+qRo0aCgoKcnU5KEAnTpzQihUrNGDAAIcG8byAgIDCLwqF7o477lD9+vW1ZMkSV5cCN0STeB2YNWuWHnroIUnnpitSUlK0Zs0aF1cFZ1m+fLl8fX3l6+ur0qVL68svv9TixYtltfLH+Xqyd+9eGYahWrVquboUuFitWrV04MABV5cBN8TfKsXc7t27tWHDBj3wwAOSpBIlSuj+++/XrFmzXFwZnKV169ZKSEhQQkKCNmzYoLZt26p9+/Y6ePCgq0tDAWI2AOcZhiGLxeLqMuCGWJNYzM2aNUvZ2dkKCwuzjxmGIZvNpnfffVf+/v4urA7O4OPjoxo1ath//uCDD+Tv76/3339fr732mgsrQ0GqWbOmLBaLdu3a5epS4GI7d+5U1apVXV0G3BBJYjGWnZ2t+fPna/z48fZkKSEhQVu3blVYWJg++ugjV5eIQmCxWGS1WpWenu7qUlCAAgMD1bZtW02dOlVpaWmm7cnJyYVfFApdXFyctm/frm7durm6FLghksRibPny5Tp58qT69u1rSgy7deumWbNm6fHHH3dRdXCWzMxMJSYmSpJOnjypd999V6mpqerUqZOLK0NBmzp1qlq0aKFbbrlFo0ePVr169ZSdna2VK1dq2rRp2rlzp6tLRAE6/2c7JydHR44c0YoVKxQbG6vo6Gj16tXL1eXBDdEkFmOzZs1SVFRUnlPK3bp107hx47Rt2zbVq1fPBdXBWVasWKHQ0FBJUunSpVWrVi19+umnatWqlWsLQ4GrVq2atmzZotdff13PPfecDh8+rHLlyqlx48aaNm2aq8tDATv/Z7tEiRIqU6aM6tevr8mTJysmJoYL0+ASFoPV0QAAALgAv5oAAADAhCYRAAAAJjSJAAAAMKFJBAAAgAlNIgAAAExoEgEAAGBCkwgAAAATmkQAAACY0CQCKLJ69+6tLl262H9u1aqVnnnmmUKv48cff5TFYuH7kgG4FZpEAPnWu3dvWSwWWSwWeXp6qkaNGho9erSys7Od+rpLlizRmDFjrmhfGjsAuDZ8dzOAq9KuXTvNmTNHmZmZ+uabbzRgwACVLFlSQ4YMcdjv7Nmz8vT0LJDXDAwMLJDjAAAujyQRwFWx2WwKCQlR5cqV9cQTTygqKkpffvmlfYr49ddfV1hYmMLDwyVJf//9t7p3766AgAAFBgaqc+fOOnDggP14OTk5Gjx4sAICAhQUFKQXX3xRF361/IXTzZmZmXrppZdUsWJF2Ww21ahRQ7NmzdKBAwfUunVrSVKZMmVksVjUu3dvSVJubq5iY2NVtWpVeXt7q379+vrss88cXuebb77RjTfeKG9vb7Vu3dqhTgBwFzSJAAqEt7e3zp49K0latWqVdu/erZUrV2r58uXKyspS27ZtVbp0af3000/63//+J19fX7Vr187+nPHjx2vu3LmaPXu2fv75Z504cUJLly695Gv26tVLH330kSZPnqydO3dqxowZ8vX1VcWKFfX5559Lknbv3q3Dhw9r0qRJkqTY2FjNnz9f06dP144dO/Tss8/qoYce0po1aySda2a7du2qTp06KSEhQY8++qhefvllZ502ACiymG4GcE0Mw9CqVav03XffadCgQTp27Jh8fHz0wQcf2KeZP/zwQ+Xm5uqDDz6QxWKRJM2ZM0cBAQH68ccfddddd2nixIkaMmSIunbtKkmaPn26vvvuu4u+7h9//KFPPvlEK1euVFRUlCSpWrVq9u3np6bLly+vgIAASeeSx7Fjx+qHH35QZGSk/Tk///yzZsyYoZYtW2ratGmqXr26xo8fL0kKDw/X9u3b9eabbxbgWQOAoo8mEcBVWb58uXx9fZWVlaXc3Fw9+OCDGjlypAYMGKC6des6rEPcunWr9u7dq9KlSzscIyMjQ3/++adSUlJ0+PBhNW3a1L6tRIkSatKkiWnK+byEhAR5eHioZcuWV1zz3r17debMGd15550O42fPnlXDhg0lSTt37nSoQ5K9oQQAd0KTCOCqtG7dWtOmTZOnp6fCwsJUosT//e/Ex8fHYd/U1FQ1btxYCxcuNB2nXLlyV/X63t7e+X5OamqqJOnrr79WhQoVHLbZbLarqgMArlc0iQCuio+Pj2rUqHFF+zZq1EiLFy9W+fLl5efnl+c+oaGhWr9+vW6//XZJUnZ2tjZv3qxGjRrluX/dunWVm5urNWvW2Keb/+t8kpmTk2Mfq127tmw2m/7666+LJpARERH68ssvHcbWrVt3+TcJANcZLlwB4HQ9e/ZU2bJl1blzZ/3000/av3+/fvzxRz311FP6559/JElPP/203njjDS1btky7du3Sk08+ecl7HFapUkUxMTF65JFHtGzZMvsxP/nkE0lS5cqVZbFYtHz5ch07dkypqakqXbq0nn/+eT377LOaN2+e/vzzT23ZskVTpkzRvHnzJEmPP/649uzZoxdeeEG7d+/WokWLNHfuXGefIgAocmgSAThdqVKltHbtWlWqVEldu3ZVRESE+vbtq4yMDHuy+Nxzz+nhhx9WTEyMIiMjVbp0ad1zzz2XPO60adN077336sknn1StWrXUr18/paWlSZIqVKigUaNG6eWXX1ZwcLAGDhwoSRozZoyGDRum2NhYRUREqF27dvr6669VtWpVSVKlSpX0+eefa9myZapfv76mT5+usWPHOvHsAEDRZDEutiocAAAAboskEQAAACY0iQAAADChSQQAAIAJTSIAAABMaBIBAABgQpMIAAAAE5pEAAAAmNAkAgAAwIQmEQAAACY0iQAAADChSQQAAIAJTSIAAABM/h/et/5coAl3lAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "true_labels = labels[0]\n",
    "predicted_labels = labels[1]\n",
    "\n",
    "# 计算混淆矩阵\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# 可视化混淆矩阵热图\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['A', 'B', 'C', 'D'],\n",
    "            yticklabels=['A', 'B', 'C', 'D'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "\n",
    "# 显示准确度\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "plt.text(1.5, -0.5, f'Accuracy: {accuracy:.2f}', ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6755973-6d18-47ec-b321-6abbaca04e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.946236559139785\n",
      "Recall: 0.8380952380952381\n",
      "F1 Score: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "# 直接进行二分类结果\n",
    "\n",
    "from utils import convert_to_binary_labels, calculate_metrics\n",
    "\n",
    "\n",
    "true_labels_bin = convert_to_binary_labels(labels[0])\n",
    "predicted_labels_bin = convert_to_binary_labels(labels[1])\n",
    "\n",
    "precision, recall, f1_score = calculate_metrics(true_labels_bin, predicted_labels_bin)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a4f95c-ab78-470c-bd90-9ccff5cfd189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
